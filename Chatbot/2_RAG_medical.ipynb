{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24763475a3d8998c",
   "metadata": {},
   "source": [
    "<h1>Model Inference with RAG</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c004acc-13cd-4917-8480-592c7c2d623b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Change that following variable settings match your deployed model's *Inference endpoint*. for example: \n",
    "\n",
    "```\n",
    "infer_endpoint = \"https://model-vllm.apps.clusterx.sandboxx.opentlc.com\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e51a248-1ca8-496a-9fe3-9306b1974b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from file 3_RAG_with_Elastic\n",
    "# client = Elasticsearch(['https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'], basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), verify_certs=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eadce4d-0b69-4f0a-aff7-5f888e85e4f3",
   "metadata": {},
   "source": [
    "<h1>Model Inference</h1>\n",
    "\n",
    "<h3>using: /v1/completions endpoint</h3>\n",
    "<h4>*More for Single-shot prompts, single-turn Q&A or document completion</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de65d02-84a6-4cff-882e-551cdd42b486",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_endpoint = \"http://model-predictor.minio.svc.cluster.local:8080\" #Change infer endpoint here\n",
    "infer_url = f\"{infer_endpoint}/v1/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc76424-b6cb-4ac9-a4d5-54a3da27d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"model\",\"object\":\"model\",\"created\":1745399297,\"owned_by\":\"vllm\",\"root\":\"/mnt/models\",\"parent\":null,\"max_model_len\":4096,\"permission\":[{\"id\":\"modelperm-6efda1d1ab644349b109915e5b0913d7\",\"object\":\"model_permission\",\"created\":1745399297,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "# get the model name \n",
    "# for testing only\n",
    "!curl http://model-predictor.minio.svc.cluster.local:8080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdff6a7-03c9-418d-ae55-22efbbc32cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenShift is a container application platform built around a core of Docker container packaging and Kubernetes orchestration. It provides an enterprise-grade container platform with originating technology contributions from Red Hat, Google, and others in the open source community.\n",
      "\n",
      "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery.\n",
      "\n",
      "In summary, OpenShift is a distribution of Kubernetes with additional features and tools tailored for enterprise use cases, while Kubernetes is the underlying orchestration engine for managing containers at scale.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# Test for hyperparameter tuning \n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"prompt\": \"What's the difference between OpenShift and Kubernetes?\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['text']\n",
    "print(generated_response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa17d77-2268-4551-bcdb-355608dd0fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenShift is an open-source container application platform based on Kubernetes. It provides a more user-friendly and feature-rich environment for developers and operations teams, including built-in CI/CD pipelines, a web-based console, and integrated monitoring tools. Kubernetes, on the other hand, is a container orchestration system that automates the deployment, scaling, and management of containerized applications. It requires more manual configuration and integration with other tools.\n"
     ]
    }
   ],
   "source": [
    "# Test for hyperparameter tuning \n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"prompt\": \"What's the difference between OpenShift and Kubernetes?\",\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['text']\n",
    "print(generated_response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61cf4b-12c7-45ab-bb8b-180bac0cdd4c",
   "metadata": {},
   "source": [
    "<h1>Model Inference</h1>\n",
    "\n",
    "<h3>using: /v1/chat/completions endpoint</h3>\n",
    "<h3>*More for chat-tuned, multi-turn conversations, model behaving like assistant</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea5b6587-e26d-4885-91a5-e774e479c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO DO into the Prompt Template\n",
    "# preserve messages history \n",
    "# keep older messages if they are essential for content \n",
    "# truncate long histories by summarizing/dropping older exchanges \n",
    "\n",
    "# Prompt token - is there a max? \n",
    "\n",
    "# create a Python class to connect to FE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1adf3582-4d44-4fd2-80f9-0fb79fca5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_endpoint = \"http://model-predictor.minio.svc.cluster.local:8080\" #Change infer endpoint here\n",
    "infer_url = f\"{infer_endpoint}/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d85782f-f1d6-40a7-b483-0098df7f5293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/SEAK_AI\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4360dbaf-baa5-41b5-9e68-078f3b9c0dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"model\",\"object\":\"model\",\"created\":1745399306,\"owned_by\":\"vllm\",\"root\":\"/mnt/models\",\"parent\":null,\"max_model_len\":4096,\"permission\":[{\"id\":\"modelperm-85d97a29967141ffb8e2faef4ecfcde3\",\"object\":\"model_permission\",\"created\":1745399306,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "# get the model name \n",
    "# for testing only\n",
    "!curl http://model-predictor.minio.svc.cluster.local:8080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a68837a-cf3d-464b-9a9a-d816c55daca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenShift and Kubernetes are both container orchestration tools, but they have some key differences:\n",
      "\n",
      "1. **Base Technology**: Kubernetes is an open-source platform designed to automate deploying, scaling, and managing containerized applications. It groups containers that make up an application into logical units for easy management and discovery. On the other hand, OpenShift is a container application platform built on top of Kubernetes. It adds additional features and capabilities.\n",
      "\n",
      "2. **Ease of Use**: OpenShift is generally considered easier to use than raw Kubernetes because it includes a web console and automation features out of the box. This can be particularly beneficial for developers who may not have extensive experience with Kubernetes. \n",
      "\n",
      "3. **Additional Features**: OpenShift comes with several built-in features like integrated CI/CD pipelines (using tools such as Jenkins), a private registry for container images, operational safety through automated backups and self-healing, and more robust role-based access control (RBAC). These extra features aim to provide a complete development and operations environment for the enterprise.\n",
      "\n",
      "4. **Support and Services**: Red Hat offers official support for OpenShift, which can be crucial for businesses needing professional assistance. While Kubernetes has a large community providing support via various channels, direct technical support isn't guaranteed unless you opt for paid services from vendors like Red Hat or Google (with GKE - Google Kubernetes Engine).\n",
      "\n",
      "5. **Customization**: Since Kubernetes is more basic and modular, it allows for greater customization based on specific needs. OpenShift, while powerful, might limit certain advanced customizations due to its added layers of abstraction and pre-configured components.\n",
      "\n",
      "In summary, if you're looking for a more straightforward setup with additional enterprise-level features right away, OpenShift could be the better choice. If you prefer a leaner tool that you can customize extensively according to your requirements and don't mind setting up these additional elements yourself, then Kubernetes would be suitable.\n"
     ]
    }
   ],
   "source": [
    "user_question_one = \"What's the difference between OpenShift & Kubernetes?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chat bot assistant for helping people with their queries.\"},\n",
    "    {\"role\": \"user\", \"content\": user_question_one}\n",
    "]\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['message']['content']\n",
    "print(generated_response.strip())\n",
    "\n",
    "# add to the messages list \n",
    "messages.append({\"role\": \"system\", \"content\": generated_response.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d0596c-62e3-4b19-b54f-a85203d985b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd be happy to explain how you can implement Retrieval-Augmented Generation (RAG) using Elasticsearch as your vector database.\n",
      "\n",
      "Retrieval-Augented Generation (RAG) is a methodology that combines the power of retrieval systems with generative models. The idea is to retrieve relevant information from a large corpus of data and feed this information into a language model to generate human-like text. Here’s a simplified step-by-step guide on how to implement this using Elasticsearch as your vector database:\n",
      "\n",
      "1. **Data Preparation**: Firstly, you need to prepare your data. This involves tokenizing your documents and converting them into vectors using techniques like word embeddings (Word2Vec, GloVe), sentence embeddings (Sentence-BERT, Universal Sentence Encoder), or contextual embeddings (ELMo, BERT). These vectors will represent your documents in a high dimensional space where semantically similar texts are close together.\n",
      "\n",
      "2. **Indexing Documents in Elasticsearch**: Once you've converted your documents into vectors, you'll index these vectors alongside their corresponding document IDs in Elasticsearch. You can use the Elasticsearch's Vector Search API for this purpose, which allows you to store and query dense vectors.\n",
      "\n",
      "   ```json\n",
      "   PUT /my_index\n",
      "   {\n",
      "     \"settings\": {\n",
      "       \"number_of_shards\": 1,\n",
      "       \"number_of_replicas\": 0\n",
      "     },\n",
      "     \"mappings\": {\n",
      "       \"properties\": {\n",
      "         \"vector\": {\n",
      "           \"type\": \"dense_vector\",\n",
      "           \"dimensions\": 512\n",
      "         },\n",
      "         \"_id\": {\n",
      "           \"type\": \"keyword\"\n",
      "         }\n",
      "       }\n",
      "     }\n",
      "   }\n",
      "   \n",
      "   POST /my_index/_doc/1\n",
      "   {\n",
      "     \"vector\": [0.1, 0.2, 0.3],\n",
      "     \"_id\": \"doc1\"\n",
      "   }\n",
      "   ```\n",
      "   \n",
      "3. **Querying with Vector Similarity**: When you want to retrieve relevant documents for a given prompt, convert the prompt into a vector in the same way as your indexed documents. Then use Elasticsearch's `more_like_this` query or `script_score` function with cosine similarity measure to find the most similar vectors (and thus documents) in your index.\n",
      "\n",
      "   ```json\n",
      "   GET /my_index/_search\n",
      "   {\n",
      "     \"query\": {\n",
      "       \"function_score\": {\n",
      "         \"functions\": [\n",
      "           {\n",
      "             \"script_score\": {\n",
      "               \"script\": {\n",
      "                 \"source\": \"cosineSimilarity(params.queryVector, 'vector') + 1.0\",\n",
      "                 \"params\": {\n",
      "                   \"queryVector\": [0.4, 0.5, 0.6] // Your prompt vector here\n",
      "                 }\n",
      "               }\n",
      "             }\n",
      "           }\n",
      "         ],\n",
      "         \"field_value_factor\": {\"field\": \"vector\", \"factor\": 1.0},\n",
      "         \"score_mode\": \"sum\"\n",
      "       }\n",
      "     }\n",
      "   }\n",
      "   ```\n",
      "\n",
      "4. **Fine-tuning Language Model**: After retrieving relevant documents, feed these into a language model fine-tuned for generation tasks (like text summarization or question answering). Fine-tune it on your specific dataset so it knows how to utilize the retrieved information effectively.\n",
      "\n",
      "5. **Generation**: Finally, use the fine-tuned language model to generate text based on the retrieved documents and the initial input prompt.\n",
      "\n",
      "Remember that each step involves complex processes and considerations depending on your specific use case and dataset size. Also, keep in mind that working with vector databases requires careful handling of dimensionality and computational resources due to the high-dimensional nature of vector spaces. \n",
      "\n",
      "This is a high level overview and actual implementation might require deeper dives into each step mentioned above, especially when dealing with details related to machine learning model training and integration with search engines like Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "# ongoing conversations \n",
    "\n",
    "user_question_two = \"I'm interested in implementing AI with RAG approach, tell me more about how i can do it with elastic as the vector database\"\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_question_two})\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['message']['content']\n",
    "print(generated_response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2a831-9d78-4159-b393-58865906f2a5",
   "metadata": {},
   "source": [
    "<h1>Create Embeddings in Elastic Vector Database</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577e04b8-11be-4c2a-a97e-b45e018127ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/app-root/lib64/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: elasticsearch in /opt/app-root/lib64/python3.11/site-packages (9.0.0)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in /opt/app-root/lib64/python3.11/site-packages (from elasticsearch) (8.17.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/app-root/lib64/python3.11/site-packages (from elasticsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /opt/app-root/lib64/python3.11/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.3.0)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil->elasticsearch) (1.17.0)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/app-root/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# install dependencies & imports\n",
    "!pip install sentence-transformers elasticsearch pandas tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from tqdm import tqdm\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d482934a-6c89-4fc9-b0f7-c448676dd479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   issue_id  answer_id author              creation_time  \\\n",
      "0    456983          0  almet  2010-12-05T17:31:55+00:00   \n",
      "1    456983          1  Gui13  2010-12-05T17:43:43+00:00   \n",
      "2    456983          2  almet  2010-12-05T17:45:28+00:00   \n",
      "3    456983          3  almet  2010-12-14T15:57:04+00:00   \n",
      "4    456983          4  Gui13  2010-12-21T21:02:19+00:00   \n",
      "\n",
      "                                          issue_body  \\\n",
      "0  Hey, Say I add an image in my article, giving ...   \n",
      "1  Hey, Say I add an image in my article, giving ...   \n",
      "2  Hey, Say I add an image in my article, giving ...   \n",
      "3  Hey, Say I add an image in my article, giving ...   \n",
      "4  Hey, Say I add an image in my article, giving ...   \n",
      "\n",
      "                                         answer_body  \n",
      "0  If we do add the SITEURL value, it can break b...  \n",
      "1  A bit hackish for sure, but I'm not yet good e...  \n",
      "2  okay, let's go for that so. Will do that when ...  \n",
      "3  arnaud is currently working on that, see his f...  \n",
      "4                     Should this bug be closed now?  \n",
      "\n",
      "\n",
      "['issue_id', 'answer_id', 'author', 'creation_time', 'issue_body', 'answer_body']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_helpdesk_data.csv\", low_memory=False)\n",
    "print(df.head())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "855b0992-a52a-4d8c-a13c-1a19506ca934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/SEAK_AI\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321accf0-c0b0-4a7e-9c33-63f1494258f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing - create a smaller csv\n",
    "df = pd.read_csv(\"cleaned_helpdesk_data.csv\")\n",
    "# df_sample = df.head(100)\n",
    "# df_sample.to_csv(\"helpdesk_small_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fedfa2b-f61a-4257-bdce-92b30c071e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'elasticsearch-sample-es-default-1', 'cluster_name': 'elasticsearch-sample', 'cluster_uuid': 'nr3Sh4ArQcqFm90rHd5Smw', 'version': {'number': '8.17.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2b6a7fed44faa321997703718f07ee0420804b41', 'build_date': '2024-12-11T12:08:05.663969764Z', 'build_snapshot': False, 'lucene_version': '9.12.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70712625-c421-4644-87f4-a407bc3d0eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   issue_id                                         issue_body  \\\n",
      "0    456983  Hey, Say I add an image in my article, giving ...   \n",
      "1    648603  if you render 5 templates deep, and that templ...   \n",
      "2    798136  I have an app which is using connect.vhost to ...   \n",
      "3    814067  Hello, There is a problem with the Pelican's l...   \n",
      "4    984126  Hi, I'm just about to migrate to pelican and f...   \n",
      "5    998441  Hi, I'm trying pelican (which is exactly what ...   \n",
      "6   2349963  The page about **data types** <link> mentions ...   \n",
      "7   2891909  I'd love support for additional Markdown file ...   \n",
      "8   3052375  SET in pelican.conf.py: `STATIC_PATHS = [\"imag...   \n",
      "9   3500667  When I create a post using Pelican, images app...   \n",
      "\n",
      "                                         answer_body  \n",
      "0  If we do add the SITEURL value, it can break b...  \n",
      "1  Having a similar issue when rendering partials...  \n",
      "2  Just so I can find it again (compare view is n...  \n",
      "3  Having a header like this one in _all_ pelican...  \n",
      "4  Hello, No, as far as i know, it's not supposed...  \n",
      "5  You've missed the blank line after the code-bl...  \n",
      "6  Hi, used to be 1GB, was later modified to 512M...  \n",
      "7  Thanks for the notice. I don't know if you're ...  \n",
      "8  I think it is, yes. The way static path and im...  \n",
      "9  I've tried something around this. When I wrote...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 09:44:22.066969: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-23 09:44:22.083293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745401462.104537    6256 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745401462.111194    6256 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745401462.127315    6256 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745401462.127334    6256 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745401462.127336    6256 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745401462.127338    6256 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-23 09:44:22.132544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/app-root/lib64/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:403: SecurityWarning: Connecting to 'https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'elasticsearch-sample-es-default-1', 'cluster_name': 'elasticsearch-sample', 'cluster_uuid': 'nr3Sh4ArQcqFm90rHd5Smw', 'version': {'number': '8.17.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2b6a7fed44faa321997703718f07ee0420804b41', 'build_date': '2024-12-11T12:08:05.663969764Z', 'build_snapshot': False, 'lucene_version': '9.12.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings stored successfully in Elasticsearch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LOGIC\n",
    "# ideally, all the contents of 'cleaned_body' field from the continous answer_ids grouped under the same issue_id should be in a single vector embedding \n",
    "\n",
    "# Load your job dataset\n",
    "# df = pd.read_csv(\"helpdesk_small_sample.csv\")\n",
    "\n",
    "# not all values in clean_body are strings, thus throwing TypeError if this line is not runned \n",
    "df[\"answer_body\"] = df[\"answer_body\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Group answers by issue_id and concatenate them in order\n",
    "grouped = (\n",
    "    df.sort_values(by=[\"issue_id\", \"answer_id\"])\n",
    "      .groupby(\"issue_id\")\n",
    "      .agg({\n",
    "          \"issue_body\": \"first\",\n",
    "          \"answer_body\": lambda x: \" \".join(x)\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(grouped.head(10))\n",
    "# Embedding Model \n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # faced out of memory error \n",
    "# model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\", device=\"cuda\")  # lighter version\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cuda')\n",
    "\n",
    "# Combine issue_body and answer_body into a single string for embedding\n",
    "grouped[\"full_text\"] = grouped[\"issue_body\"] + \" \" + grouped[\"answer_body\"]\n",
    "\n",
    "# Generate embeddings\n",
    "grouped[\"embedding\"] = model.encode(grouped[\"full_text\"]).tolist()\n",
    "\n",
    "# grouped = df.sort_values(by=[\"issue_id\", \"answer_id\"]).groupby(\"issue_id\")[\"clean_body\"].apply(lambda texts: \" \".join(texts)).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "print(es.info())\n",
    "\n",
    "index_name = \"helpdesk-embeddings\"\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"issue_id\": {\"type\": \"keyword\"},\n",
    "                    \"issue_body\": {\"type\": \"text\"},\n",
    "                    \"answer_body\": {\"type\": \"text\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 384,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Prepare docs for bulk indexing\n",
    "def create_docs(data):\n",
    "    for _, row in data.iterrows():\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": row[\"issue_id\"],\n",
    "            \"_source\": {\n",
    "                \"issue_id\": row[\"issue_id\"],\n",
    "                \"issue_body\": row[\"issue_body\"],\n",
    "                \"answer_body\": row[\"answer_body\"],\n",
    "                \"embedding\": row[\"embedding\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Bulk upload\n",
    "bulk(es, create_docs(grouped))\n",
    "print(\"✅ Embeddings stored successfully in Elasticsearch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08058696-f67e-4aba-8f1a-80bbaadb5bfa",
   "metadata": {},
   "source": [
    "### Adding Medical Document using docling as new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf83ba06-97d3-48c4-b070-35884699574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docling in /opt/app-root/lib64/python3.11/site-packages (2.34.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/app-root/lib64/python3.11/site-packages (from docling) (4.13.3)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /opt/app-root/lib64/python3.11/site-packages (from docling) (2025.1.31)\n",
      "Requirement already satisfied: click<8.2.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (8.1.8)\n",
      "Requirement already satisfied: docling-core<3.0.0,>=2.29.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-core[chunking]<3.0.0,>=2.29.0->docling) (2.31.2)\n",
      "Requirement already satisfied: docling-ibm-models<4.0.0,>=3.4.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (3.4.3)\n",
      "Requirement already satisfied: docling-parse<5.0.0,>=4.0.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (4.0.1)\n",
      "Requirement already satisfied: easyocr<2.0,>=1.7 in /opt/app-root/lib64/python3.11/site-packages (from docling) (1.7.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (1.2.0)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in /opt/app-root/lib64/python3.11/site-packages (from docling) (0.30.2)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (5.4.0)\n",
      "Requirement already satisfied: marko<3.0.0,>=2.1.2 in /opt/app-root/lib64/python3.11/site-packages (from docling) (2.1.3)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /opt/app-root/lib64/python3.11/site-packages (from docling) (3.1.5)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /opt/app-root/lib64/python3.11/site-packages (from docling) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (11.2.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (1.6.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (2.11.3)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (2.9.1)\n",
      "Requirement already satisfied: pylatexenc<3.0,>=2.10 in /opt/app-root/lib64/python3.11/site-packages (from docling) (2.10)\n",
      "Requirement already satisfied: pypdfium2<5.0.0,>=4.30.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (4.30.1)\n",
      "Requirement already satisfied: python-docx<2.0.0,>=1.1.2 in /opt/app-root/lib64/python3.11/site-packages (from docling) (1.1.2)\n",
      "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in /opt/app-root/lib64/python3.11/site-packages (from docling) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /opt/app-root/lib64/python3.11/site-packages (from docling) (2.32.3)\n",
      "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (1.4.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (1.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /opt/app-root/lib64/python3.11/site-packages (from docling) (4.67.1)\n",
      "Requirement already satisfied: typer<0.16.0,>=0.12.5 in /opt/app-root/lib64/python3.11/site-packages (from docling) (0.15.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/app-root/lib64/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/app-root/lib64/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.12.2)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-core<3.0.0,>=2.29.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (1.1.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-core<3.0.0,>=2.29.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (4.23.0)\n",
      "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-core<3.0.0,>=2.29.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (3.78.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from docling-core<3.0.0,>=2.29.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (6.0.2)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-core<3.0.0,>=2.29.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (0.9.0)\n",
      "Requirement already satisfied: semchunk<3.0.0,>=2.2.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-core[chunking]<3.0.0,>=2.29.0->docling) (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-core[chunking]<3.0.0,>=2.29.0->docling) (4.51.3)\n",
      "Requirement already satisfied: jsonlines<4.0.0,>=3.1.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (3.1.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.24.4 in /opt/app-root/lib64/python3.11/site-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (2.2.5)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /opt/app-root/lib64/python3.11/site-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (4.11.0.86)\n",
      "Requirement already satisfied: safetensors<1,>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from safetensors[torch]<1,>=0.4.3->docling-ibm-models<4.0.0,>=3.4.0->docling) (0.5.3)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.2.2 in /opt/app-root/lib64/python3.11/site-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (2.7.0)\n",
      "Requirement already satisfied: torchvision<1,>=0 in /opt/app-root/lib64/python3.11/site-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (0.22.0)\n",
      "Requirement already satisfied: scikit-image in /opt/app-root/lib64/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (0.25.2)\n",
      "Requirement already satisfied: python-bidi in /opt/app-root/lib64/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (0.6.6)\n",
      "Requirement already satisfied: Shapely in /opt/app-root/lib64/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (2.1.1)\n",
      "Requirement already satisfied: pyclipper in /opt/app-root/lib64/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in /opt/app-root/lib64/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (1.11.1.4)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling) (24.2)\n",
      "Requirement already satisfied: et-xmlfile in /opt/app-root/lib64/python3.11/site-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.1.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /opt/app-root/lib64/python3.11/site-packages (from python-pptx<2.0.0,>=1.0.2->docling) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling) (1.26.20)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/app-root/lib64/python3.11/site-packages (from typer<0.16.0,>=0.12.5->docling) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/app-root/lib64/python3.11/site-packages (from typer<0.16.0,>=0.12.5->docling) (14.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/app-root/lib64/python3.11/site-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4.0.0,>=3.4.0->docling) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.29.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.29.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.29.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.11/site-packages (from rich>=10.11.0->typer<0.16.0,>=0.12.5->docling) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.11/site-packages (from rich>=10.11.0->typer<0.16.0,>=0.12.5->docling) (2.19.1)\n",
      "Requirement already satisfied: mpire[dill] in /opt/app-root/lib64/python3.11/site-packages (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (2.10.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/app-root/lib64/python3.11/site-packages (from triton==3.3.0->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (74.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/app-root/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (0.21.1)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /opt/app-root/lib64/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/app-root/lib64/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2025.5.26)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/app-root/lib64/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.16.0,>=0.12.5->docling) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from sympy>=1.13.3->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.0.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /opt/app-root/lib64/python3.11/site-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (0.70.18)\n",
      "Requirement already satisfied: dill>=0.4.0 in /opt/app-root/lib64/python3.11/site-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.29.0->docling) (0.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adff0ebb-0423-4492-b288-3906bc482c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 125 RAG-ready chunks to 'parsed_medical_chunks.jsonl' and markdown to 'parsed_medical_report.md'\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "\n",
    "pdf_path = \"UMNwriteup.pdf\"\n",
    "\n",
    "entries = []\n",
    "documents = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    current_left = \"\"\n",
    "    current_comment = \"\"\n",
    "    entry_open = False\n",
    "    last_left_end_char = \"\"\n",
    "    col_cutoff = 320  # adjust as needed\n",
    "\n",
    "    for page in pdf.pages:\n",
    "        words = page.extract_words()\n",
    "        words.sort(key=lambda w: (round(float(w['top']), 1), float(w['x0'])))\n",
    "\n",
    "        # Group by Y\n",
    "        lines = []\n",
    "        current_line_y = None\n",
    "        current_line_words = []\n",
    "        for w in words:\n",
    "            top = round(float(w['top']), 1)\n",
    "            if current_line_y is None or abs(top - current_line_y) > 0.5:\n",
    "                if current_line_words:\n",
    "                    lines.append(current_line_words)\n",
    "                current_line_words = [w]\n",
    "                current_line_y = top\n",
    "            else:\n",
    "                current_line_words.append(w)\n",
    "        if current_line_words:\n",
    "            lines.append(current_line_words)\n",
    "\n",
    "        for line_words in lines:\n",
    "            line_words.sort(key=lambda w: float(w['x0']))\n",
    "            left_text_line = \"\"\n",
    "            right_text_line = \"\"\n",
    "            for w in line_words:\n",
    "                text = w['text'].strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                if float(w['x0']) < col_cutoff:\n",
    "                    left_text_line += \" \" + text if left_text_line else text\n",
    "                else:\n",
    "                    right_text_line += \" \" + text if right_text_line else text\n",
    "            left_text_line = left_text_line.strip()\n",
    "            right_text_line = right_text_line.strip()\n",
    "\n",
    "            if left_text_line == \"History and Physical Examination\" and right_text_line == \"Comments\":\n",
    "                continue\n",
    "            if left_text_line.startswith(\"Microsoft Word\"):\n",
    "                continue\n",
    "\n",
    "            if left_text_line:\n",
    "                if not entry_open:\n",
    "                    current_left = left_text_line\n",
    "                    current_comment = \"\"\n",
    "                    entry_open = True\n",
    "                else:\n",
    "                    continuation = False\n",
    "                    if last_left_end_char not in \".!?\":\n",
    "                        if left_text_line and not left_text_line[0].isupper():\n",
    "                            continuation = True\n",
    "                        if re.match(r'^(and|but|or|nor|so|for|yet)\\b', left_text_line, flags=re.IGNORECASE):\n",
    "                            continuation = True\n",
    "                        if last_left_end_char in \"-,;\":\n",
    "                            continuation = True\n",
    "                    if re.match(r'^\\d+\\.', left_text_line):\n",
    "                        continuation = False\n",
    "                    if not continuation:\n",
    "                        entries.append((current_left.strip(), current_comment.strip() if current_comment else \"\"))\n",
    "                        current_left = left_text_line\n",
    "                        current_comment = \"\"\n",
    "                    else:\n",
    "                        if current_left.endswith(\"-\"):\n",
    "                            current_left = current_left[:-1] + left_text_line\n",
    "                        else:\n",
    "                            current_left += \" \" + left_text_line\n",
    "                last_left_end_char = current_left[-1] if current_left else \"\"\n",
    "                if right_text_line:\n",
    "                    current_comment += (\" \" if current_comment else \"\") + right_text_line\n",
    "            elif right_text_line:\n",
    "                current_comment += (\" \" if current_comment else \"\") + right_text_line\n",
    "\n",
    "    if entry_open:\n",
    "        entries.append((current_left.strip(), current_comment.strip() if current_comment else \"\"))\n",
    "\n",
    "# Post-processing known issue fix\n",
    "for i, (left, comment) in enumerate(entries):\n",
    "    if \"y/o WF\" in left and \"who has been\" in comment:\n",
    "        left = left.replace(\"WF having\", \"WF who has been having\")\n",
    "        comment = comment.replace(\"who has been\", \"\").strip()\n",
    "        entries[i] = (left, comment)\n",
    "\n",
    "# --- Save outputs for RAG ---\n",
    "\n",
    "# Save as Markdown file (optional)\n",
    "with open(\"parsed_medical_report.md\", \"w\", encoding=\"utf-8\") as md_file:\n",
    "    for left, comment in entries:\n",
    "        if left:\n",
    "            md_file.write(left + \"\\n\")\n",
    "            if comment:\n",
    "                md_file.write(f\"> _{comment}_\\n\")\n",
    "            md_file.write(\"\\n\")\n",
    "\n",
    "# Save as JSONL for embedding\n",
    "with open(\"parsed_medical_chunks.jsonl\", \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "    for left, comment in entries:\n",
    "        doc = {\n",
    "            \"content\": left.strip() + (\"\\n\\nComment:\\n\" + comment.strip() if comment else \"\"),\n",
    "            \"metadata\": {\n",
    "                \"source\": \"UMNwriteup.pdf\"\n",
    "            }\n",
    "        }\n",
    "        documents.append(doc)\n",
    "        jsonl_file.write(json.dumps(doc) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Saved {len(documents)} RAG-ready chunks to 'parsed_medical_chunks.jsonl' and markdown to 'parsed_medical_report.md'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "286cb052-2302-4cc0-b41d-35a4700fe33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 20.21it/s]\n",
      "/opt/app-root/lib64/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:397: SecurityWarning: Connecting to 'https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_63344/2623412841.py:36: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  es.indices.create(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created index 'medical-rag-embeddings'.\n",
      "⬆️ Uploading to Elasticsearch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded 125 documents to 'medical-rag-embeddings'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "# === CONFIG ===\n",
    "jsonl_path = \"parsed_medical_chunks.jsonl\"  # path to your jsonl file\n",
    "index_name = \"medical-rag-embeddings\"       # new Elasticsearch index name\n",
    "model_name = \"multi-qa-MiniLM-L6-cos-v1\"    # embedding model\n",
    "embedding_dim = 384                         # dimension for the model above\n",
    "\n",
    "# === Load JSONL to DataFrame ===\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    docs = [json.loads(line) for line in f]\n",
    "\n",
    "df = pd.DataFrame(docs)\n",
    "df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n",
    "\n",
    "# === Generate Embeddings ===\n",
    "print(\"🔍 Generating embeddings...\")\n",
    "model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "df[\"embedding\"] = model.encode(df[\"content\"].tolist(), show_progress_bar=True).tolist()\n",
    "\n",
    "# === Connect to Elasticsearch ===\n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"],\n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# === Create Index with Mapping ===\n",
    "if es.indices.exists(index=index_name):\n",
    "    print(f\"⚠️ Index '{index_name}' already exists. Choose another name or delete it.\")\n",
    "else:\n",
    "    es.indices.create(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"content\": {\"type\": \"text\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": embedding_dim,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    },\n",
    "                    \"metadata\": {\"type\": \"object\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(f\"✅ Created index '{index_name}'.\")\n",
    "\n",
    "# === Prepare documents for bulk upload ===\n",
    "def create_docs(dataframe):\n",
    "    for i, row in dataframe.iterrows():\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": f\"doc-{i}\",\n",
    "            \"_source\": {\n",
    "                \"content\": row[\"content\"],\n",
    "                \"embedding\": row[\"embedding\"],\n",
    "                \"metadata\": row.get(\"metadata\", {})  # safe fallback\n",
    "            }\n",
    "        }\n",
    "\n",
    "# === Upload ===\n",
    "print(\"⬆️ Uploading to Elasticsearch...\")\n",
    "bulk(es, create_docs(df))\n",
    "print(f\"✅ Uploaded {len(df)} documents to '{index_name}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21eb03fd-5651-4ac7-b465-f9ace6dc7210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available indices: ['medical-rag-embeddings', 'helpdesk-embeddings']\n",
      "Num of embeddings in index: 15784\n",
      "ID: doc-0\n",
      "ID: doc-1\n",
      "ID: doc-2\n",
      "ID: doc-3\n",
      "ID: doc-4\n",
      "ID: doc-5\n",
      "ID: doc-6\n",
      "ID: doc-7\n",
      "ID: doc-8\n",
      "ID: doc-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## for debugging/testing purposes only \n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# view available embedding indices \n",
    "indices = es.indices.get_alias().keys()\n",
    "print(\"Available indices:\", list(indices))\n",
    "\n",
    "# view all stored embeddings \n",
    "results = es.search(\n",
    "    index=\"medical-rag-embeddings\",\n",
    "    size=10,  # adjust as needed\n",
    "    _source=[\"embedding\"]  # only return embedding field\n",
    ")\n",
    "\n",
    "# view the number of documents in the stored index \n",
    "count = es.count(index=\"helpdesk-embeddings\")[\"count\"]\n",
    "print(f\"Num of embeddings in index: {count}\")\n",
    "\n",
    "\n",
    "for doc in results[\"hits\"][\"hits\"]:\n",
    "    print(f\"ID: {doc['_id']}\")\n",
    "    # print(\"Embedding:\", doc[\"_source\"][\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4668850a-653f-4f22-9a9a-f0ce82a366dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available indices: ['medical-rag-embeddings', 'helpdesk-embeddings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# delete the entire index after checking \n",
    "# ONLY RUN TO DELETE THE ENTIRE INDEX CREATED - SO THAT IT IS A CLEAN STATE IN VECTOR DB\n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# es.indices.delete(index=\"helpdesk-embeddings\", ignore_unavailable=True)\n",
    "# print(\"Index deleted.\\n\")\n",
    "\n",
    "# view available embedding indices \n",
    "indices = es.indices.get_alias().keys()\n",
    "print(\"Available indices:\", list(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a5b52-dad1-4f56-ba1a-8f1e79107c86",
   "metadata": {},
   "source": [
    "<h1>Retrieving Top 3 Most Relevant Embeddings from Vector DB (Helpdesk)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e944a9a-a2eb-492d-8277-441287a299c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'dense_vector', 'dims': 384, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "mapping = es.indices.get_mapping(index=\"helpdesk-embeddings\")\n",
    "print(mapping[\"helpdesk-embeddings\"][\"mappings\"][\"properties\"].get(\"embedding\", \"Field not found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0b37886-768f-47ec-bbe3-84974c9f5941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'dense_vector', 'dims': 384, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "mapping = es.indices.get_mapping(index=\"medical-rag-embeddings\")\n",
    "print(mapping[\"medical-rag-embeddings\"][\"mappings\"][\"properties\"].get(\"embedding\", \"Field not found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07b4b153-1630-46d1-97ef-f28ca7b92e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:403: SecurityWarning: Connecting to 'https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue ID: 814067\n",
      "Answer Body: Having a header like this one in _all_ pelican's files seems more than annoying to me. After some search, it seems that the recommended practice is to to add the notice in every file, but not doing so will not invalidate the license or change the meaning of it (<link> Anyway, I will change the LICENSE text. > After some search, it seems that the recommended practice is to to add the notice in every file, but not doing so will not invalidate the license Interesting, I did not know.\n",
      "Score: 0.7950542\n",
      "==================================================\n",
      "Issue ID: 1267992009\n",
      "Answer Body: `pelican` doesn't have a `parser.py`. This is likely from a plugin. Try removing plugins or running `pelican` with `--debug` to see which file/plugin is the issue. ![image](<link> I don't recall installing this plugin. I'm currently in dependency hell, can't get linker plugin to import a module from itself. Hey David. Were you eventually able to get everything sorted out?\n",
      "Score: 0.7678268\n",
      "==================================================\n",
      "Issue ID: 683970143\n",
      "Answer Body: Hi Michael. * `from 'pelican.utils'` indicates that it's trying to import from within Pelican — not Six or some other dependency * searching for `python_2_unicode_compatible` inside Pelican's code base yields nothing, so it was likely removed * searching `pelican/utils.py` history shows [where `python_2_unicode_compatible` used to be](<link> * Pelican core no longer supports Python 2.7; this is squarely a plugin-related concern * submoduled is a far cry from officially maintained, which is why we encourage folks to move to [Pelican Plugins](<link> If you would like to continue using the `pelican_toc` plugin, and you are interested in helping with its modernization and migration to the new organization, would you please open an issue in the [legacy Pelican plugins repo](<link> as noted in the [Pelican 4.5 release notes](<link>\n",
      "Score: 0.7424195\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import urllib3\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Disable SSL warnings\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cpu')\n",
    "\n",
    "\n",
    "# get top 3 most relevant embeddings \n",
    "def search_embeddings(query, top_n=3):\n",
    "    # Step 1: Embed the query using the same model\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "\n",
    "    # Step 2: Search the Elasticsearch index for the closest vector matches\n",
    "    response = es.search(\n",
    "        index=\"helpdesk-embeddings\",\n",
    "        body={\n",
    "            \"size\": top_n,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"field\": \"embedding\",\n",
    "                    \"k\": top_n,\n",
    "                    \"num_candidates\": 100,\n",
    "                    \"query_vector\": query_embedding\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    relevant_answers = []\n",
    "\n",
    "    for hit in hits:\n",
    "        issue_id = hit[\"_source\"][\"issue_id\"]\n",
    "        answer_body = hit[\"_source\"][\"answer_body\"]\n",
    "        score = hit[\"_score\"]\n",
    "        relevant_answers.append({\n",
    "            \"issue_id\": issue_id,\n",
    "            \"answer_body\": answer_body,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    return relevant_answers\n",
    "\n",
    "user_query = \"I have an issue with Pelican license, where pelican doe not have a parser.py\"\n",
    "top_matches = search_embeddings(user_query) \n",
    "\n",
    "for match in top_matches:\n",
    "    print(f\"Issue ID: {match['issue_id']}\")\n",
    "    print(f\"Answer Body: {match['answer_body']}\")\n",
    "    print(f\"Score: {match['score']}\")\n",
    "    print(\"=\"*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ec97e-77b7-4bd7-8424-3d73559f5ef9",
   "metadata": {},
   "source": [
    "<h1>Retrieving Top 3 Most Relevant Embeddings from Vector DB (Medical)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ae3de5a-3e7b-4f09-b67c-017fe999419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:397: SecurityWarning: Connecting to 'https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Match 1:\n",
      "Content:\n",
      "This is the first admission for this 56 year old woman, who states she was in her usual state of good health until one week prior to admission. At that time she noticed the abrupt onset (over a few seconds to a minute) of chest pain which she describes as dull and aching in character. The pain began in the left para-sternal area and radiated up to her neck. The first episode of pain one week ago occurred when she was working in her garden in the middle of the day. She states she had been working for approximately 45 minutes and began to feel tired before the onset of the pain.\n",
      "\n",
      "Comment:\n",
      "Convey the acute or chronic nature of the problem and establish a chronology. onset character location radiation circumstances; exacerbating factors\n",
      "Metadata: {'source': 'UMNwriteup.pdf'}\n",
      "Score: 0.8182\n",
      "================================================================================\n",
      "🔹 Match 2:\n",
      "Content:\n",
      "1. Carefully monitor the patient for any increased chest pain that might be indicative of impending myocardial infarction by admitting the patient to the telemetry floor.\n",
      "\n",
      "Comment:\n",
      "You should develop a diagnostic and therapeutic plan for the patient. Your plan should incorporate acute and long-term care of the patient’s most likely problem. You should consider pharmacologic and non-pharmacologic\n",
      "Metadata: {'source': 'UMNwriteup.pdf'}\n",
      "Score: 0.8167\n",
      "================================================================================\n",
      "🔹 Match 3:\n",
      "Content:\n",
      "This patient’s description of dull, aching, exertion related substernal chest pain is suggestive of ischemic cardiac origin. Her findings of a FH of early ASCVD, hypertension, and early surgical menopause are pertinent risk factors for development of coronary artery disease. Therefore, the combination of this patient’s presentation, and the multiple risk factors make angina pectoris the most likely diagnosis. The pain symptoms appear to be increasing, and the occurrence of pain at rest suggests this fits the presentation of unstable angina, and hospitalization is indicated.\n",
      "\n",
      "Comment:\n",
      "In the assessment you take each of the patient’s major problems and draw conclusions, in this case that the chest pain is more likely due to ischemic heart disease instead of other possibilities. You tie in several of the other problems as risk factors for ischemic heart disease, and not merely as random unrelated problems. You should list and extensive justification for your most likely diagnosis. You should also explain why you are less suspicious of alternative diagnoses, such as esophageal reflux disease,\n",
      "Metadata: {'source': 'UMNwriteup.pdf'}\n",
      "Score: 0.8118\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63344/782220014.py:25: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.search(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import urllib3\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Disable SSL warnings\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cpu')\n",
    "\n",
    "# === Search Function for JSONL-RAG ===\n",
    "def search_embeddings(query, top_n=3):\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "\n",
    "    response = es.search(\n",
    "        index=\"medical-rag-embeddings\",  # update this if your index name differs\n",
    "        body={\n",
    "            \"size\": top_n,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"field\": \"embedding\",\n",
    "                    \"k\": top_n,\n",
    "                    \"num_candidates\": 100,\n",
    "                    \"query_vector\": query_embedding\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    results = []\n",
    "\n",
    "    for hit in hits:\n",
    "        source = hit[\"_source\"]\n",
    "        content = source.get(\"content\", \"\")\n",
    "        metadata = source.get(\"metadata\", {})\n",
    "        score = hit[\"_score\"]\n",
    "\n",
    "        results.append({\n",
    "            \"content\": content,\n",
    "            \"metadata\": metadata,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Example Query ===\n",
    "user_query = \"A patient presents with chest pain radiating to the neck\"\n",
    "top_matches = search_embeddings(user_query)\n",
    "\n",
    "# === Display Results ===\n",
    "for i, match in enumerate(top_matches, 1):\n",
    "    print(f\"🔹 Match {i}:\")\n",
    "    print(f\"Content:\\n{match['content']}\")\n",
    "    print(f\"Metadata: {match['metadata']}\")\n",
    "    print(f\"Score: {match['score']:.4f}\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485b6d1-238d-44e8-9a61-6672d33ab38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39175fab-a3f5-4f58-a4a9-c7b295775b91",
   "metadata": {},
   "source": [
    "<h1>Model Inference - Prompt Construction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d82db52-2afc-47d6-9396-b8954047d383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for the confusion, but the provided context doesn't include any information related to Elasticsearch errors. However, I can still try to help you troubleshoot this issue based on common practices.\n",
      "\n",
      "The error message `BadRequestError: BadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Accept, Content-Type]')` typically occurs when there's an issue with the HTTP headers sent by your client (in this case, presumably your Elasticsearch script or application). \n",
      "\n",
      "Here are a few things you could check:\n",
      "\n",
      "1. **Check Accept Header**: The `Accept` header specifies what content types the client understands. Ensure that it's set correctly. For example, if you're expecting JSON responses, it should look something like this: `Accept: application/json`.\n",
      "\n",
      "2. **Check Content-Type Header**: The `Content-Type` header tells Elasticsearch what format the data being sent is in. If you're sending data in JSON format, it should be set to `application/json`.\n",
      "\n",
      "3. **Ensure Correct Encoding**: Make sure that your request body (if any) is properly encoded in UTF-8. Some libraries handle encoding automatically, while others require manual setting.\n",
      "\n",
      "4. **Update Elasticsearch Client Library**: If you're using a third-party library to interact with Elasticsearch, ensure it's up-to-date. Older versions might have bugs causing such issues.\n",
      "\n",
      "If none of these suggestions resolve your issue, consider checking Elasticsearch's official documentation or reaching out to their community support channels for further assistance. They'd be better equipped to provide specific guidance given their expertise with their own product.\n"
     ]
    }
   ],
   "source": [
    "infer_endpoint = \"http://model-predictor.minio.svc.cluster.local:8080\" #Change infer endpoint here\n",
    "infer_url = f\"{infer_endpoint}/v1/chat/completions\"\n",
    "\n",
    "user_query = \"I have the error BadRequestError: BadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Accept, Content-Type]') when running elasticsearch\"\n",
    "# Build the retrieved context\n",
    "context = \"\\n\\n---\\n\\n\".join([f\"Issue ID: {match['issue_id']}\\nAnswer: {match['answer_body']}\" for match in top_matches])\n",
    "\n",
    "# Step 2: Construct messages for /v1/chat/completions (include chain-of-thought reasoning model)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful and knowledgeable support assistant. \"\n",
    "            \"Answer the user's question clearly and accurately. \"\n",
    "            \"Use the provided context from our database to support your response wherever relevant. \"\n",
    "            \"Think step by step when forming your answer, reasoning through each part of the problem before responding. \"\n",
    "            \"If the context does not contain enough information to answer the question, \"\n",
    "            \"politely let the user know and suggest alternative next steps if appropriate.\\n\\n\"\n",
    "            f\"Context:\\n{context}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_query\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "# query LLM server\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['message']['content']\n",
    "print(generated_response.strip())\n",
    "\n",
    "# add to the messages list \n",
    "messages.append({\"role\": \"system\", \"content\": generated_response.strip()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f9ece-e9cf-44e2-a8a2-73160186aee8",
   "metadata": {},
   "source": [
    "<h1>Request Function</h1>\n",
    "\n",
    "Build and submit the REST request. \n",
    "\n",
    "Note: You submit the data in the same format that you used for an ONNX inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9386f-683a-4880-b780-c40bec3ab9f8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def rest_request(prompt):\n",
    "    json_data = {\n",
    "        \"model\": \"llm\",\n",
    "        \"prompt\": [\n",
    "            prompt\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"logprobs\": 0,\n",
    "        \"echo\": False,\n",
    "        \"stop\": [\n",
    "            \"string\"\n",
    "        ],\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"best_of\": 1,\n",
    "        \"user\": \"string\",\n",
    "        \"top_k\": -1,\n",
    "        \"ignore_eos\": False,\n",
    "        \"use_beam_search\": False,\n",
    "        \"stop_token_ids\": [\n",
    "            0\n",
    "        ],\n",
    "        \"skip_special_tokens\": True,\n",
    "        \"spaces_between_special_tokens\": True,\n",
    "        \"repetition_penalty\": 1,\n",
    "        \"min_p\": 0,\n",
    "        \"include_stop_str_in_output\": False,\n",
    "        \"length_penalty\": 1\n",
    "    }\n",
    "\n",
    "    response = requests.post(infer_url, json=json_data, verify=False)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad16ac-23da-48bd-9796-f8e4cacae981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = rest_request(\"What accelerators are supported in openshift AI?\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd2c336d-44a7-47cc-81ea-ab14b980fc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!curl http://model-predictor.minio.svc.cluster.local:8080/health"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
