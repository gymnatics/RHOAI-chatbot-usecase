{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24763475a3d8998c",
   "metadata": {},
   "source": [
    "<h1>Model Inference with RAG Implementation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eadce4d-0b69-4f0a-aff7-5f888e85e4f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Model Inference</h1>\n",
    "\n",
    "<h3>using: /v1/completions endpoint</h3>\n",
    "<h4>*More for Single-shot prompts, single-turn Q&A or document completion</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d532194-2cf2-4eee-b79c-2640bc1f9eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 2)) (2.2.5)\n",
      "Collecting seaborn (from -r requirements.txt (line 3))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting matplotlib (from -r requirements.txt (line 4))\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting missingno (from -r requirements.txt (line 5))\n",
      "  Downloading missingno-0.5.2-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: scipy in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 6)) (1.15.2)\n",
      "Collecting statsmodels (from -r requirements.txt (line 7))\n",
      "  Downloading statsmodels-0.14.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 8)) (1.6.1)\n",
      "Requirement already satisfied: nltk in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 9)) (3.9.1)\n",
      "Requirement already satisfied: sentence-transformers in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 10)) (4.1.0)\n",
      "Requirement already satisfied: transformers in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 11)) (4.51.3)\n",
      "Requirement already satisfied: accelerate in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 12)) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 4))\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 4))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 4))\n",
      "  Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 4))\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib->-r requirements.txt (line 4)) (11.2.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 4))\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting patsy>=0.5.6 (from statsmodels->-r requirements.txt (line 7))\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.11/site-packages (from nltk->-r requirements.txt (line 9)) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/app-root/lib64/python3.11/site-packages (from nltk->-r requirements.txt (line 9)) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from nltk->-r requirements.txt (line 9)) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 10)) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 10)) (0.30.2)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 10)) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from transformers->-r requirements.txt (line 11)) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from transformers->-r requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from transformers->-r requirements.txt (line 11)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/app-root/lib64/python3.11/site-packages (from transformers->-r requirements.txt (line 11)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from transformers->-r requirements.txt (line 11)) (0.5.3)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib64/python3.11/site-packages (from accelerate->-r requirements.txt (line 12)) (6.1.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 10)) (2025.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/app-root/lib64/python3.11/site-packages (from triton==3.3.0->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (74.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers->-r requirements.txt (line 11)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers->-r requirements.txt (line 11)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers->-r requirements.txt (line 11)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers->-r requirements.txt (line 11)) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 10)) (3.0.2)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m363.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading missingno-0.5.2-py3-none-any.whl (8.7 kB)\n",
      "Downloading statsmodels-0.14.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m341.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m400.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m650.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, patsy, kiwisolver, fonttools, cycler, contourpy, matplotlib, statsmodels, seaborn, missingno\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 missingno-0.5.2 patsy-1.0.1 pyparsing-3.2.3 seaborn-0.13.2 statsmodels-0.14.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing required libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "689c2b40-826b-451d-8dbf-f72fa0603f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from tqdm import tqdm\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de65d02-84a6-4cff-882e-551cdd42b486",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_endpoint = \"http://model-predictor.minio.svc.cluster.local:8080\" #Change infer endpoint here\n",
    "infer_url = f\"{infer_endpoint}/v1/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc76424-b6cb-4ac9-a4d5-54a3da27d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"model\",\"object\":\"model\",\"created\":1745841436,\"owned_by\":\"vllm\",\"root\":\"/mnt/models\",\"parent\":null,\"max_model_len\":4096,\"permission\":[{\"id\":\"modelperm-ca5dee3fcbe5471da4109e7dc0a168ea\",\"object\":\"model_permission\",\"created\":1745841436,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "# get the model name \n",
    "# for testing only\n",
    "!curl http://model-predictor.minio.svc.cluster.local:8080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bdff6a7-03c9-418d-ae55-22efbbc32cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenShift is a container application platform built around a core of Docker container packaging and Kubernetes orchestration. It adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, multi-cloud deployments, and portability across cloud providers. In other words, OpenShift is a distribution of Kubernetes that includes additional features and tools for developers and operations teams.\n"
     ]
    }
   ],
   "source": [
    "# Test for hyperparameter tuning \n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"prompt\": \"What's the difference between OpenShift and Kubernetes?\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['text']\n",
    "print(generated_response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fa17d77-2268-4551-bcdb-355608dd0fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenShift and Kubernetes are both open-source container orchestration systems, but they have some key differences:\n",
      "\n",
      "1. **Origin and Development**: Kubernetes was originally developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). OpenShift, on the other hand, was developed by Red Hat and is based on Kubernetes.\n",
      "\n",
      "2. **Ease of Use**: OpenShift is designed to be more user-friendly and easier to set up, especially for developers who are not Kubernetes experts. It provides a web console, CLI tools, and integrated development environments (IDEs) for managing applications. Kubernetes, while powerful, can be more complex to set up and manage, requiring a deeper understanding of its components and APIs.\n",
      "\n",
      "3. **Built-in Features**: OpenShift comes with several built-in features that are not part of the standard Kubernetes distribution. These include:\n",
      "   - **Source-to-Image (S2I)**: A\n"
     ]
    }
   ],
   "source": [
    "# Test for hyperparameter tuning \n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"prompt\": \"What's the difference between OpenShift and Kubernetes?\",\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['text']\n",
    "print(generated_response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61cf4b-12c7-45ab-bb8b-180bac0cdd4c",
   "metadata": {},
   "source": [
    "<h1>Model Inference</h1>\n",
    "\n",
    "<h3>using: /v1/chat/completions endpoint</h3>\n",
    "<h3>*More for chat-tuned, multi-turn conversations, model behaving like assistant</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5b6587-e26d-4885-91a5-e774e479c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO DO into the Prompt Template\n",
    "# preserve messages history \n",
    "# keep older messages if they are essential for content \n",
    "# truncate long histories by summarizing/dropping older exchanges \n",
    "\n",
    "# Prompt token - is there a max? \n",
    "\n",
    "# create a Python class to connect to FE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1adf3582-4d44-4fd2-80f9-0fb79fca5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_endpoint = \"http://model-predictor.minio.svc.cluster.local:8080\" #Change infer endpoint here\n",
    "infer_url = f\"{infer_endpoint}/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4360dbaf-baa5-41b5-9e68-078f3b9c0dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"model\",\"object\":\"model\",\"created\":1745841624,\"owned_by\":\"vllm\",\"root\":\"/mnt/models\",\"parent\":null,\"max_model_len\":4096,\"permission\":[{\"id\":\"modelperm-46acfada92ca428e9477a3c99a8587ec\",\"object\":\"model_permission\",\"created\":1745841624,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# get the model name \n",
    "# for testing only\n",
    "!curl http://model-predictor.minio.svc.cluster.local:8080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a68837a-cf3d-464b-9a9a-d816c55daca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenShift and Kubernetes are both containerization platforms, but they have some key differences:\n",
      "\n",
      "1. **Origin and Ownership**: Kubernetes (K8s) is an open-source platform developed by Google based on their internal system Borg. It's managed by the Cloud Native Computing Foundation (CNCF). On the other hand, OpenShift is a commercial product from Red Hat (now part of IBM), built on top of Kubernetes.\n",
      "\n",
      "2. **Ease of Use**: OpenShift is often considered more user-friendly than Kubernetes because it offers a more streamlined experience out of the box. It includes features like automated setup, integrated development environment tools, and simplified application deployment processes. \n",
      "\n",
      "3. **Additional Features**: OpenShift comes with additional features that aren't included in standard Kubernetes, such as a web console for managing clusters, built-in continuous integration/continuous delivery (CI/CD) pipelines, and integrated image registry. \n",
      "\n",
      "4. **Support and Documentation**: Being a commercial product, OpenShift provides enterprise-level support and extensive documentation directly from Red Hat/IBM. While Kubernetes also has robust community support, finding specific solutions to problems might require more effort compared to OpenShift.\n",
      "\n",
      "5. **Customizability**: Because Kubernetes is more basic and modular, it can be customized extensively according to specific needs. This level of customization isn't always necessary or desired by all users, especially those who prefer a more straightforward solution like OpenShift.\n",
      "\n",
      "6. **Pricing**: Since OpenShift is a commercial product, there are costs associated with its use, particularly for larger deployments or enterprise customers requiring support. Kubernetes itself is free to use but may incur operational costs if you're running it on cloud infrastructure.\n",
      "\n",
      "In summary, while Kubernetes provides a powerful foundation for orchestrating containers at scale, OpenShift builds upon this with additional tools and ease-of-use features that make it ideal for enterprises looking for a more comprehensive platform for developing and deploying containerized applications.\n"
     ]
    }
   ],
   "source": [
    "user_question_one = \"What's the difference between OpenShift & Kubernetes?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chat bot assistant for helping people with their queries.\"},\n",
    "    {\"role\": \"user\", \"content\": user_question_one}\n",
    "]\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['message']['content']\n",
    "print(generated_response.strip())\n",
    "\n",
    "# add to the messages list \n",
    "messages.append({\"role\": \"system\", \"content\": generated_response.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d0596c-62e3-4b19-b54f-a85203d985b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing AI with Retrieval-Augmented Generation (RAG) using Elasticsearch as the vector database involves several steps. Here's a simplified guide to help you get started:\n",
      "\n",
      "1. **Data Preparation**: Firstly, you need to prepare your data. Your dataset should consist of text passages that will serve as your knowledge base. These texts will be converted into vectors (numerical representations) that can be stored and queried in Elasticsearch.\n",
      "\n",
      "2. **Vectorization**: Convert your text data into numerical vectors using techniques like Word2Vec, GloVe, or Sentence Transformers' universal sentence encoder. These methods convert words or sentences into high-dimensional vectors where semantically similar items are close together in the vector space.\n",
      "\n",
      "3. **Indexing Vectors in Elasticsearch**: After generating vectors, you'll need to index them in Elasticsearch. You can store these vectors directly in Elasticsearch documents alongside their corresponding textual representation. To facilitate fast similarity search, consider using Elasticsearch's `doc_values` feature for storing dense tensors (vector data).\n",
      "\n",
      "4. **Building RAG Model**: The RAG model consists of two main components - a retriever and a generator.\n",
      "\n",
      "   - **Retriever**: This component takes a query (also converted into a vector), searches through your indexed vectors, and retrieves the most relevant ones from your knowledge base. In the context of Elasticsearch, you'd write a script or use its Query DSL to perform this task efficiently.\n",
      "   \n",
      "   - **Generator**: Given the retrieved relevant passages (as input), the generator uses a pre-trained language model (like BERT, T5, etc.) to generate an output based on these inputs. This is typically done using a sequence-to-sequence model fine-tuned on your specific task (e.g., question answering, text generation).\n",
      "\n",
      "5. **Integration**: Integrate the retriever and generator components into a single pipeline. When you receive a new query, first pass it through the retriever to find relevant passages in your knowledge base, then feed these passages into the generator to produce the final response.\n",
      "\n",
      "6. **Training and Fine-tuning**: Train your RAG model end-to-end on your specific task using appropriate datasets. Fine-tune your language model on your domain-specific corpus to improve performance.\n",
      "\n",
      "7. **Evaluation and Iteration**: Continuously evaluate your model's performance and iterate over improvements – this could involve tuning hyperparameters, adjusting retrieval strategies, or refining your training data.\n",
      "\n",
      "Remember that setting up such a system requires expertise in machine learning, natural language processing, and working with distributed systems like Elasticsearch. There are also various libraries and frameworks available that can assist with different parts of this process, such as Hugging Face's Transformers for building models and working with pre-trained language models, and Elastic's AI Assistant for simplifying AI integration with Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "# ongoing conversations \n",
    "\n",
    "user_question_two = \"I'm interested in implementing AI with RAG approach, tell me more about how i can do it with elastic as the vector database\"\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_question_two})\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['message']['content']\n",
    "print(generated_response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2a831-9d78-4159-b393-58865906f2a5",
   "metadata": {},
   "source": [
    "<h1>Create Embeddings in Elastic Vector Database</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d482934a-6c89-4fc9-b0f7-c448676dd479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   issue_id  answer_id author              creation_time  \\\n",
      "0    456983          0  almet  2010-12-05T17:31:55+00:00   \n",
      "1    456983          1  Gui13  2010-12-05T17:43:43+00:00   \n",
      "2    456983          2  almet  2010-12-05T17:45:28+00:00   \n",
      "3    456983          3  almet  2010-12-14T15:57:04+00:00   \n",
      "4    456983          4  Gui13  2010-12-21T21:02:19+00:00   \n",
      "\n",
      "                                          issue_body  \\\n",
      "0  Hey, Say I add an image in my article, giving ...   \n",
      "1  Hey, Say I add an image in my article, giving ...   \n",
      "2  Hey, Say I add an image in my article, giving ...   \n",
      "3  Hey, Say I add an image in my article, giving ...   \n",
      "4  Hey, Say I add an image in my article, giving ...   \n",
      "\n",
      "                                         answer_body  \n",
      "0  If we do add the SITEURL value, it can break b...  \n",
      "1  A bit hackish for sure, but I'm not yet good e...  \n",
      "2  okay, let's go for that so. Will do that when ...  \n",
      "3  arnaud is currently working on that, see his f...  \n",
      "4                     Should this bug be closed now?  \n",
      "\n",
      "\n",
      "['issue_id', 'answer_id', 'author', 'creation_time', 'issue_body', 'answer_body']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_helpdesk_data.csv\", low_memory=False)\n",
    "print(df.head())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "321accf0-c0b0-4a7e-9c33-63f1494258f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing - create a smaller csv\n",
    "df = pd.read_csv(\"cleaned_helpdesk_data.csv\")\n",
    "# df_sample = df.head(100)\n",
    "# df_sample.to_csv(\"helpdesk_small_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04de91ab-5db7-4c63-8e7b-e657219216fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"elasticsearch-sample-es-default-0\",\n",
      "  \"cluster_name\" : \"elasticsearch-sample\",\n",
      "  \"cluster_uuid\" : \"nr3Sh4ArQcqFm90rHd5Smw\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"8.17.0\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"docker\",\n",
      "    \"build_hash\" : \"2b6a7fed44faa321997703718f07ee0420804b41\",\n",
      "    \"build_date\" : \"2024-12-11T12:08:05.663969764Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"9.12.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"7.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -u elastic:1rdzVXU8i8F9T8hs1p6c78d6 -k https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7cd0326-9f82-42fc-b98d-7ea02433131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch==8.10.0 in /opt/app-root/lib64/python3.11/site-packages (8.10.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in /opt/app-root/lib64/python3.11/site-packages (from elasticsearch==8.10.0) (8.17.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /opt/app-root/lib64/python3.11/site-packages (from elastic-transport<9,>=8->elasticsearch==8.10.0) (1.26.20)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from elastic-transport<9,>=8->elasticsearch==8.10.0) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install elasticsearch==8.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f949044c-088d-436e-a755-a20d1986c106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: elasticsearch\n",
      "Version: 8.10.0\n",
      "Summary: Python client for Elasticsearch\n",
      "Home-page: https://github.com/elastic/elasticsearch-py\n",
      "Author: Elastic Client Library Maintainers\n",
      "Author-email: client-libs@elastic.co\n",
      "License: Apache-2.0\n",
      "Location: /opt/app-root/lib64/python3.11/site-packages\n",
      "Requires: elastic-transport\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d9ff48-4c59-4c47-a746-0aad8a9bbcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4d1bad3-9747-4271-9f3e-220f3b3a852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'name': 'elasticsearch-sample-es-default-1', 'cluster_name': 'elasticsearch-sample', 'cluster_uuid': 'nr3Sh4ArQcqFm90rHd5Smw', 'version': {'number': '8.17.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2b6a7fed44faa321997703718f07ee0420804b41', 'build_date': '2024-12-11T12:08:05.663969764Z', 'build_snapshot': False, 'lucene_version': '9.12.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# elasticsearch client \n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "url = \"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"\n",
    "auth = HTTPBasicAuth(\"elastic\", \"1rdzVXU8i8F9T8hs1p6c78d6\")\n",
    "\n",
    "response = requests.get(url, auth=auth, verify=False)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8218c35e-c4e7-4176-9c74-41719cde497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'elasticsearch-sample-es-default-1', 'cluster_name': 'elasticsearch-sample', 'cluster_uuid': 'nr3Sh4ArQcqFm90rHd5Smw', 'version': {'number': '8.17.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2b6a7fed44faa321997703718f07ee0420804b41', 'build_date': '2024-12-11T12:08:05.663969764Z', 'build_snapshot': False, 'lucene_version': '9.12.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:397: SecurityWarning: Connecting to 'https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"],\n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "print(es.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "70712625-c421-4644-87f4-a407bc3d0eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:403: SecurityWarning: Connecting to 'https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'elasticsearch-sample-es-default-1', 'cluster_name': 'elasticsearch-sample', 'cluster_uuid': 'nr3Sh4ArQcqFm90rHd5Smw', 'version': {'number': '8.17.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2b6a7fed44faa321997703718f07ee0420804b41', 'build_date': '2024-12-11T12:08:05.663969764Z', 'build_snapshot': False, 'lucene_version': '9.12.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n",
      "✅ Embeddings stored successfully in Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "# LOGIC\n",
    "# ideally, all the contents of 'cleaned_body' field from the continous answer_ids grouped under the same issue_id should be in a single vector embedding \n",
    "\n",
    "# Load your job dataset\n",
    "# df = pd.read_csv(\"helpdesk_small_sample.csv\")\n",
    "\n",
    "# not all values in clean_body are strings, thus throwing TypeError if this line is not runned \n",
    "df[\"answer_body\"] = df[\"answer_body\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Group answers by issue_id and concatenate them in order\n",
    "grouped = (\n",
    "    df.sort_values(by=[\"issue_id\", \"answer_id\"])\n",
    "      .groupby(\"issue_id\")\n",
    "      .agg({\n",
    "          \"issue_body\": \"first\",\n",
    "          \"answer_body\": lambda x: \" \".join(x)\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Combine issue_body and answer_body into a single string for embedding\n",
    "grouped[\"full_text\"] = grouped[\"issue_body\"] + \" \" + grouped[\"answer_body\"]\n",
    "\n",
    "# Generate embeddings\n",
    "grouped[\"embedding\"] = model.encode(grouped[\"full_text\"]).tolist()\n",
    "\n",
    "# grouped = df.sort_values(by=[\"issue_id\", \"answer_id\"]).groupby(\"issue_id\")[\"clean_body\"].apply(lambda texts: \" \".join(texts)).reset_index()\n",
    "\n",
    "\n",
    "# Embedding Model \n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # faced out of memory error \n",
    "# model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\", device=\"cuda\")  # lighter version\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cpu')\n",
    "\n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "print(es.info())\n",
    "\n",
    "index_name = \"helpdesk-embeddings\"\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"issue_id\": {\"type\": \"keyword\"},\n",
    "                    \"issue_body\": {\"type\": \"text\"},\n",
    "                    \"answer_body\": {\"type\": \"text\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 384,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Prepare docs for bulk indexing\n",
    "def create_docs(data):\n",
    "    for _, row in data.iterrows():\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": row[\"issue_id\"],\n",
    "            \"_source\": {\n",
    "                \"issue_id\": row[\"issue_id\"],\n",
    "                \"issue_body\": row[\"issue_body\"],\n",
    "                \"answer_body\": row[\"answer_body\"],\n",
    "                \"embedding\": row[\"embedding\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Bulk upload\n",
    "bulk(es, create_docs(grouped))\n",
    "print(\"✅ Embeddings stored successfully in Elasticsearch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21eb03fd-5651-4ac7-b465-f9ace6dc7210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available indices: ['helpdesk-embeddings']\n",
      "Num of embeddings in index: 15784\n",
      "ID: 681691498\n",
      "ID: 681841359\n",
      "ID: 681859169\n",
      "ID: 681995997\n",
      "ID: 682210172\n",
      "ID: 682251668\n",
      "ID: 682255425\n",
      "ID: 682558878\n",
      "ID: 682672037\n",
      "ID: 682682405\n"
     ]
    }
   ],
   "source": [
    "## for debugging/testing purposes only \n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# view available embedding indices \n",
    "indices = es.indices.get_alias().keys()\n",
    "print(\"Available indices:\", list(indices))\n",
    "\n",
    "# view all stored embeddings \n",
    "results = es.search(\n",
    "    index=\"helpdesk-embeddings\",\n",
    "    size=10,  # adjust as needed\n",
    "    _source=[\"embedding\"]  # only return embedding field\n",
    ")\n",
    "\n",
    "# view the number of documents in the stored index \n",
    "count = es.count(index=\"helpdesk-embeddings\")[\"count\"]\n",
    "print(f\"Num of embeddings in index: {count}\")\n",
    "\n",
    "\n",
    "for doc in results[\"hits\"][\"hits\"]:\n",
    "    print(f\"ID: {doc['_id']}\")\n",
    "    # print(\"Embedding:\", doc[\"_source\"][\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4668850a-653f-4f22-9a9a-f0ce82a366dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available indices: ['helpdesk-embeddings']\n"
     ]
    }
   ],
   "source": [
    "# delete the entire index after checking \n",
    "# ONLY RUN TO DELETE THE ENTIRE INDEX CREATED - SO THAT IT IS A CLEAN STATE IN VECTOR DB\n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# es.indices.delete(index=\"helpdesk-embeddings\", ignore_unavailable=True)\n",
    "# print(\"Index deleted.\\n\")\n",
    "\n",
    "# view available embedding indices \n",
    "indices = es.indices.get_alias().keys()\n",
    "print(\"Available indices:\", list(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a5b52-dad1-4f56-ba1a-8f1e79107c86",
   "metadata": {},
   "source": [
    "<h1>Retrieving Top 3 Most Relevant Embeddings from Vector DB</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f87dfcb3-96cf-47e5-8953-4ee9e5f0f418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'dense_vector', 'dims': 384, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}\n"
     ]
    }
   ],
   "source": [
    "# elasticsearch client \n",
    "\n",
    "import os\n",
    "\n",
    "es_user = os.environ.get(\"elastic_user\")\n",
    "es_password = os.environ.get(\"elastic_password\")\n",
    "\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=(es_user, es_password), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "mapping = es.indices.get_mapping(index=\"helpdesk-embeddings\")\n",
    "print(mapping[\"helpdesk-embeddings\"][\"mappings\"][\"properties\"].get(\"embedding\", \"Field not found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e944a9a-a2eb-492d-8277-441287a299c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'dense_vector', 'dims': 384, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}\n"
     ]
    }
   ],
   "source": [
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "mapping = es.indices.get_mapping(index=\"helpdesk-embeddings\")\n",
    "print(mapping[\"helpdesk-embeddings\"][\"mappings\"][\"properties\"].get(\"embedding\", \"Field not found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07b4b153-1630-46d1-97ef-f28ca7b92e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:397: SecurityWarning: Connecting to 'https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue ID: 1386456451\n",
      "Answer Body: This might be related to . > 👍 Hi . Which version of the Arduino IDE are you using? If you haven't already, please give the latest nightly build of Arduino IDE 2.x a try and then let us know whether the problem still occurs. The download links are listed here: <link> > Hi . Which version of the Arduino IDE are you using? > > If you haven't already, please give the latest nightly build of Arduino IDE 2.x a try and then let us know whether the problem still occurs. The download links are listed here: > > <link> Hello, I am using Arduino IDE 1.8.19, I'll try that version 2.0, thanks. (I've tried this 2.0 in the past, but still haven't had esp8266 OTA working via mDNS, I use [esp-link](<link> as UNO Wifi) Hi . Did you ever get the chance to give Arduino IDE 2.x a try? If so, did the problem still occur using that version? Hi, I tested the IDE 2.0 and this problem did not occur. I noticed a long delay for the IDE 2.0 to show the \"About\" window. Is there any way to monitor the resources this IDE is using to find out if the IDE is worth using? Apparently I don't see any advantage in not using the IDE 1.8.19. Furthermore, it (IDE 2.0) still does not support upload via TCP/IP connection with automatic device search by mDNS, for ESP8266 and similar. >I tested the IDE 2.0 and this problem did not occur. Excellent. Thanks so much for checking! >I noticed a long delay for the IDE 2.0 to show the \"About\" window. That issue is tracked here: <link> >Is there any way to monitor the resources this IDE is using to find out if the IDE is worth using? Sure. You can do that just as you would with any application. If you would like guidance in doing so, please post on the Arduino forum and I'll be happy to help you out: <link> But note this specific instance of the slow dialog opening is not really a matter of resource usage as we normally think of it. It is a matter of sending an HTTP request and then waiting for a response.\n",
      "Score: 0.72427535\n",
      "==================================================\n",
      "Issue ID: 548444009\n",
      "Answer Body: An issue with a testmerged PR that was recently fixed\n",
      "Score: 0.71029353\n",
      "==================================================\n",
      "Issue ID: 2390346935\n",
      "Answer Body: Was supposed to be fixed by <link> Sigh If that is true, I would like to know whether or not it was tested because <link> is definitely not fixed. Unable to reproduce on local so yes my PR did fix it. <link> > the current server update is as recent as today. This is not fixed. Clearly this isn't the case. please check again Not only do you not have a round id, which is now a requirement for issues. There is a video demonstrating that it is fixed on latest master. Turns out I just played on the literal last round before the server was merged to add the fix. I just checked the changelog and saw it was there. > Not only do you not have a round id, which is now a requirement for issues. Just to clarify this, if I do actually find something outside of a round, would I need to at least provide evidence to demonstrate the existence of an issue?\n",
      "Score: 0.7061448\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2302/4199514187.py:25: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.search(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import urllib3\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Disable SSL warnings\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cpu')\n",
    "\n",
    "\n",
    "# get top 3 most relevant embeddings \n",
    "def search_embeddings(query, top_n=3):\n",
    "    # Step 1: Embed the query using the same model\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "\n",
    "    # Step 2: Search the Elasticsearch index for the closest vector matches\n",
    "    response = es.search(\n",
    "        index=\"helpdesk-embeddings\",\n",
    "        body={\n",
    "            \"size\": top_n,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"field\": \"embedding\",\n",
    "                    \"k\": top_n,\n",
    "                    \"num_candidates\": 100,\n",
    "                    \"query_vector\": query_embedding\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    relevant_answers = []\n",
    "\n",
    "    for hit in hits:\n",
    "        issue_id = hit[\"_source\"][\"issue_id\"]\n",
    "        answer_body = hit[\"_source\"][\"answer_body\"]\n",
    "        score = hit[\"_score\"]\n",
    "        relevant_answers.append({\n",
    "            \"issue_id\": issue_id,\n",
    "            \"answer_body\": answer_body,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    return relevant_answers\n",
    "\n",
    "user_query = \"Show me issue id \tissue_id 10060186\"\n",
    "top_matches = search_embeddings(user_query) \n",
    "\n",
    "for match in top_matches:\n",
    "    print(f\"Issue ID: {match['issue_id']}\")\n",
    "    print(f\"Answer Body: {match['answer_body']}\")\n",
    "    print(f\"Score: {match['score']}\")\n",
    "    print(\"=\"*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2ff5f3c-28f0-4a42-855f-c2cfcaa19008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from tqdm import tqdm\n",
    "from elasticsearch.helpers import bulk\n",
    "# model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cpu')\n",
    "# model.save(\"multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39175fab-a3f5-4f58-a4a9-c7b295775b91",
   "metadata": {},
   "source": [
    "<h1>Model Inference - Prompt Construction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d82db52-2afc-47d6-9396-b8954047d383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 User Question Summary:\n",
      "The user is asking for guidance on creating an application using Angular.\n",
      "\n",
      "🔵 Context Match Analysis:\n",
      "The provided context does not contain any information directly related to creating applications with Angular. It consists of discussions about Pelican, a static site generator, and its related issues such as licensing notices, parser errors, and locale settings.\n",
      "\n",
      "🔵 Final Answer:\n",
      "I'm sorry for the inconvenience, but the given context doesn't cover tutorials or guides on building applications with Angular. However, I can provide you with a basic outline to start learning Angular development:\n",
      "\n",
      "1. **Set up your environment**: Install Node.js and npm (Node Package Manager) if you haven't already. Then, install Angular CLI globally via npm (`npm install -g @angular/cli`).\n",
      "\n",
      "2. **Create a new project**: Use the Angular CLI to generate a new project (`ng new my-app`), where 'my-app' is the name of your application. Navigate into your new project directory (`cd my-app`).\n",
      "\n",
      "3. **Understand the structure**: Familiarize yourself with the project structure generated by Angular CLI. Key folders include `src`, `node_modules`, and `package.json`.\n",
      "\n",
      "4. **Learn core concepts**: Some fundamental Angular concepts include components, directives, services, modules, dependency injection, data binding, and routing. The official Angular documentation provides comprehensive guides on these topics: https://angular.io/docs\n",
      "\n",
      "5. **Build your app**: Start developing your application by adding components, implementing features, and styling with CSS or SCSS. You can use Angular Material (https://material.angular.io/) for pre-built UI components if needed.\n",
      "\n",
      "6. **Testing & Deployment**: Write tests for your components and services using Karma and Jasmine or Jest. Once satisfied with your application, build it for production (`ng build --prod`) before deploying it to a hosting service like Firebase, Netlify, or GitHub Pages.\n",
      "\n",
      "Would you like me to continue?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "#Query\n",
    "user_query = \"Can you teach me how to make an application using angular?\"\n",
    "# Step 1: Build the retrieved context\n",
    "context = \"\\n\\n---\\n\\n\".join([\n",
    "    f\"Issue ID: {match['issue_id']}\\nAnswer: {match['answer_body']}\" for match in top_matches\n",
    "])\n",
    "\n",
    "# Step 2: Construct messages for /v1/chat/completions\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a highly capable GitHub Helpdesk Support Assistant designed to assist users based on real GitHub issue threads.\\n\\n\"\n",
    "            \"🔵 Carefully follow these specific instructions:\\n\\n\"\n",
    "            \"1. Read the user question and the provided \\\"Context\\\" carefully.\\n\"\n",
    "            \"2. Think step-by-step using chain-of-thought reasoning:\\n\"\n",
    "            \"    - Summarize the user's question into clear problem keywords first.\\n\"\n",
    "            \"    - Identify whether relevant information exists in the provided Context.\\n\"\n",
    "            \"    - If sufficient context is found, proceed to answer the question, citing the related parts briefly.\\n\"\n",
    "            \"    - If no relevant context is found, politely say so and suggest the user refine or expand their question.\\n\"\n",
    "            \"3. Structure your response into these sections explicitly:\\n\"\n",
    "            \"    - **User Question Summary**\\n\"\n",
    "            \"    - **Context Match Analysis**\\n\"\n",
    "            \"    - **Final Answer**\\n\"\n",
    "            \"4. After each section output, STOP and ask the user:\\n\"\n",
    "            \"    - \\\"Would you like me to continue?\\\"\\n\"\n",
    "            \"5. Proceed only if the user explicitly agrees.\\n\\n\"\n",
    "            \"🔵 Additional Strict Rules:\\n\"\n",
    "            \"- Never hallucinate answers if context is missing.\\n\"\n",
    "            \"- If unsure, always say so politely.\\n\"\n",
    "            \"- Pause after each major step for user feedback.\\n\\n\"\n",
    "            \"---\\n\\n\"\n",
    "            f\"Context:\\n{context}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_query\n",
    "    }\n",
    "]\n",
    "\n",
    "# Step 3: Construct payload\n",
    "payload = {\n",
    "    \"model\": \"model\",  # Replace with your actual model ID\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"presence_penalty\": 0.2,\n",
    "    \"frequency_penalty\": 0.2,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Step 4: Query the LLM server\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# Step 5: Process and print output\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['message']['content']\n",
    "print(generated_response.strip())\n",
    "\n",
    "# Step 6: Optionally append the generated response back into the conversation history\n",
    "messages.append({\"role\": \"system\", \"content\": generated_response.strip()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51ae58-4580-4f3c-8b5f-d9609f681ad3",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc85ce0-2739-42c2-b0ad-9e69a3627bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import re\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Step 0: Initialize\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": (\n",
    "#             \"You are a highly capable GitHub Helpdesk Support Assistant designed to assist users based on real GitHub issue threads.\\n\\n\"\n",
    "#             \"Always maintain a helpful, professional tone.\\n\\n\"\n",
    "#             \"Carefully read the user's question.\\n\\n\"\n",
    "#             \"Use context if available.\\n\\n\"\n",
    "#             \"Only suggest clarifying questions or general troubleshooting advice if specifically instructed.\"\n",
    "#         )\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cpu')\n",
    "\n",
    "# # Step 1: Helper functions\n",
    "# def build_context(top_matches):\n",
    "#     return \"\\n\\n---\\n\\n\".join([\n",
    "#         f\"{match['answer_body']}\" for match in top_matches\n",
    "#     ])\n",
    "\n",
    "# def summarize_old_messages(messages, preserve_n=20):\n",
    "#     if len(messages) <= preserve_n:\n",
    "#         return messages\n",
    "    \n",
    "#     old_conversation = []\n",
    "#     for msg in messages[1:-preserve_n]:\n",
    "#         if msg['role'] in ['user', 'assistant']:\n",
    "#             old_conversation.append(f\"{msg['role'].capitalize()}: {msg['content']}\")\n",
    "    \n",
    "#     summary = \"🔵 Summary of earlier conversation:\\n\" + \"\\n\".join(\n",
    "#         f\"- {line}\" for line in old_conversation\n",
    "#     )\n",
    "    \n",
    "#     summarized_messages = [\n",
    "#         messages[0],\n",
    "#         {\"role\": \"system\", \"content\": summary}\n",
    "#     ] + messages[-preserve_n:]\n",
    "    \n",
    "#     return summarized_messages\n",
    "\n",
    "# def collapse_conversation_memory(messages):\n",
    "#     conversation_summary = []\n",
    "#     for msg in messages[1:]:\n",
    "#         if msg['role'] in ['user', 'assistant']:\n",
    "#             conversation_summary.append(f\"{msg['role'].capitalize()}: {msg['content']}\")\n",
    "    \n",
    "#     final_summary = \"🔵 Resolved Issue Summary:\\n\" + \"\\n\".join(\n",
    "#         f\"- {line}\" for line in conversation_summary\n",
    "#     )\n",
    "    \n",
    "#     collapsed_messages = [\n",
    "#         messages[0],\n",
    "#         {\"role\": \"system\", \"content\": final_summary}\n",
    "#     ]\n",
    "    \n",
    "#     return collapsed_messages\n",
    "\n",
    "# def get_embedding(text):\n",
    "#     return model.encode(text)\n",
    "\n",
    "# def calculate_similarity(embedding1, embedding2):\n",
    "#     embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "#     embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "#     return cosine_similarity(embedding1, embedding2)[0][0]\n",
    "\n",
    "# # Step 2: State Variables\n",
    "# initial_topic_embedding = None\n",
    "# context_injected = False\n",
    "# guiding_questions_done = False\n",
    "# clarification_rounds = 0\n",
    "\n",
    "# # Step 3: Core function\n",
    "# def send_message(user_query):\n",
    "#     global messages\n",
    "#     global initial_topic_embedding\n",
    "#     global context_injected\n",
    "#     global guiding_questions_done\n",
    "#     global clarification_rounds\n",
    "\n",
    "#     user_embedding = get_embedding(user_query)\n",
    "\n",
    "#     # Detect topic change only if input is substantial\n",
    "#     if initial_topic_embedding is None:\n",
    "#         initial_topic_embedding = user_embedding\n",
    "#     elif len(user_query.split()) > 5:  # Ignore very short replies like \"it\", \"1.8.19\"\n",
    "#         similarity = calculate_similarity(initial_topic_embedding, user_embedding)\n",
    "#         if similarity < 0.5:\n",
    "#             print(f\"🔄 Major topic change detected (similarity {similarity:.2f}). Retriggering RAG...\")\n",
    "#             context_injected = False\n",
    "#             initial_topic_embedding = user_embedding\n",
    "#             guiding_questions_done = False\n",
    "#             clarification_rounds = 0\n",
    "\n",
    "#     if not context_injected:\n",
    "#         top_matches = search_embeddings(user_query)\n",
    "#         min_score_threshold = 0.4\n",
    "\n",
    "#         if not top_matches or top_matches[0]['score'] < min_score_threshold:\n",
    "#             context_message = {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": (\n",
    "#                     \"🔵 Context Update:\\n\\n\"\n",
    "#                     \"No strong matching past issues found.\\n\\n\"\n",
    "#                     \"(Answer politely based on general knowledge.)\"\n",
    "#                 )\n",
    "#             }\n",
    "#         else:\n",
    "#             context = build_context(top_matches)\n",
    "#             quoted_context = context.replace('\\n', '\\n> ')\n",
    "#             context_message = {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": (\n",
    "#                     \"🔵 Context Update:\\n\\n\"\n",
    "#                     \"Summaries of past similar issues (use carefully):\\n\\n\"\n",
    "#                     f\"> {quoted_context}\\n\\n\"\n",
    "#                     \"(Only use if truly matching.)\"\n",
    "#                 )\n",
    "#             }\n",
    "        \n",
    "#         messages.append(context_message)\n",
    "#         context_injected = True\n",
    "\n",
    "#     # Step 2: Behavior Control\n",
    "#     user_text_lower = user_query.lower()\n",
    "#     response_behavior = \"normal\"\n",
    "\n",
    "#     vague_keywords = [\"problem\", \"idk\", \"not sure\", \"nothing working\"]\n",
    "\n",
    "#     if any(vague in user_text_lower for vague in vague_keywords):\n",
    "#         if not guiding_questions_done:\n",
    "#             response_behavior = \"guiding_questions\"\n",
    "#             guiding_questions_done = True\n",
    "#         else:\n",
    "#             clarification_rounds += 1\n",
    "#             if clarification_rounds >= 2:\n",
    "#                 response_behavior = \"general_causes\"\n",
    "#             else:\n",
    "#                 response_behavior = \"follow_up_question\"\n",
    "\n",
    "#     # Step 3: Insert dynamic system message to guide LLM\n",
    "#     if response_behavior == \"guiding_questions\":\n",
    "#         behavior_instruction = {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": (\n",
    "#                 \"🔵 Special Behavior Instruction:\\n\"\n",
    "#                 \"The user seems unsure. Kindly ask around 5 short guiding questions to clarify their issue.\"\n",
    "#             )\n",
    "#         }\n",
    "#     elif response_behavior == \"follow_up_question\":\n",
    "#         behavior_instruction = {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": (\n",
    "#                 \"🔵 Special Behavior Instruction:\\n\"\n",
    "#                 \"The user provided partial information. Ask 1 short follow-up question to get more detail.\"\n",
    "#             )\n",
    "#         }\n",
    "#     elif response_behavior == \"general_causes\":\n",
    "#         behavior_instruction = {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": (\n",
    "#                 \"🔵 Special Behavior Instruction:\\n\"\n",
    "#                 \"The user remains unclear after multiple tries. Politely suggest some general possible causes based on common troubleshooting experience.\"\n",
    "#             )\n",
    "#         }\n",
    "#     else:\n",
    "#         behavior_instruction = None\n",
    "\n",
    "#     if behavior_instruction:\n",
    "#         messages.append(behavior_instruction)\n",
    "\n",
    "#     # Step 4: Add user query\n",
    "#     messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "#     # Step 5: Build payload\n",
    "#     payload = {\n",
    "#         \"model\": \"model\",  # <-- Keep \"model\" exactly as you requested\n",
    "#         \"messages\": messages,\n",
    "#         \"max_tokens\": 512,\n",
    "#         \"temperature\": 0.3,\n",
    "#         \"top_p\": 1,\n",
    "#         \"n\": 1,\n",
    "#         \"repetition_penalty\": 1.1,\n",
    "#         \"presence_penalty\": 0.2,\n",
    "#         \"frequency_penalty\": 0.2,\n",
    "#         \"stream\": False\n",
    "#     }\n",
    "\n",
    "#     response = requests.post(infer_url, json=payload)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         output_body = response.json()\n",
    "#         generated_response = output_body['choices'][0]['message']['content']\n",
    "        \n",
    "#         tokens_generated = len(generated_response.split())\n",
    "#         if tokens_generated > 300:\n",
    "#             generated_response += \"\\n\\nWould you like me to continue?\"\n",
    "\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": generated_response.strip()})\n",
    "\n",
    "#         if len(messages) > 30:\n",
    "#             messages = summarize_old_messages(messages, preserve_n=15)\n",
    "        \n",
    "#         return generated_response.strip()\n",
    "#     else:\n",
    "#         return f\"⚠️ Error {response.status_code}: {response.text}\"\n",
    "\n",
    "# # Step 4: User interaction loop\n",
    "# print(\"🗨️ Welcome to GitHub Helpdesk Chatbot!\")\n",
    "# print(\"Type 'exit' to end the conversation.\\n\")\n",
    "\n",
    "# while True:\n",
    "#     user_query = input(\"👤 You: \")\n",
    "#     if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "#         print(\"👋 Ending the chat. Goodbye!\")\n",
    "#         break\n",
    "    \n",
    "#     if any(keyword in user_query.lower() for keyword in [\"solved\", \"fixed\", \"thank you\", \"thanks\", \"resolved\", \"issue resolved\"]):\n",
    "#         print(\"✅ Issue marked as solved! Collapsing conversation memory...\")\n",
    "#         messages = collapse_conversation_memory(messages)\n",
    "#         continue\n",
    "    \n",
    "#     bot_reply = send_message(user_query)\n",
    "#     print(f\"🤖 Assistant: {bot_reply}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb2d10-d337-48cd-81c3-dd37853f02cd",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a75fcbf-9790-4bd4-bddc-c62275d56b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗨️ Welcome to GitHub Helpdesk Chatbot!\n",
      "Type 'exit' to end the conversation.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👤 You:  hello bitch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2302/4199514187.py:25: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Assistant: 🔵 I'm here to help! However, I'd appreciate it if you could rephrase your request in a more constructive manner so I can better understand and assist you with your GitHub issue. Could you please tell me what specific problem you're encountering? Thank you.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👤 You:  I have an issue with my arduino ide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Major topic change detected (similarity 0.10). Retriggering RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2302/4199514187.py:25: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Assistant: 🔵 I'm sorry to hear that you're experiencing issues with your Arduino IDE. To better assist you, could you please provide more details about the problem? Here are some questions to guide you:\n",
      "\n",
      "1. What specific error messages are you encountering?\n",
      "2. Are you using a particular board (e.g., Uno, Mega, etc.)?\n",
      "3. Have you recently updated your Arduino IDE or its libraries?\n",
      "4. Can you describe the steps leading up to the issue?\n",
      "5. What operating system are you using (Windows, macOS, Linux)?\n",
      "\n",
      "Your answers will help me understand the situation better and propose an appropriate solution. Thank you!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👤 You:  IT keeps freezig when I unplug it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Major topic change detected (similarity 0.16). Retriggering RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2302/4199514187.py:25: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Assistant: 🔵 I see. This issue might be related to the Arduino IDE trying to access a serial port that's no longer connected when you unplug your device. Here are a few steps to troubleshoot:\n",
      "\n",
      "1. Check if the correct board and port are selected in your Arduino IDE under Tools > Board and Tools > Port.\n",
      "2. Make sure you've properly closed any sketches before disconnecting your board.\n",
      "3. Try updating your Arduino IDE to the latest stable version (1.8.19 at the moment).\n",
      "4. Disable the \"Verify Code Before Upload\" option in File > Preferences > Verify Before Upload, then enable it again.\n",
      "5. If none of these work, consider uninstalling and reinstalling the Arduino IDE, ensuring you delete any leftover files or folders associated with previous installations.\n",
      "\n",
      "Please let me know how it goes or if you need further assistance!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👤 You:  It's solved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Issue marked as solved! Collapsing conversation memory...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👤 You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Ending the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 0: Initialize\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a highly capable GitHub Helpdesk Support Assistant designed to assist users based on real GitHub issue threads.\\n\\n\"\n",
    "            \"Always maintain a professional and helpful tone.\\n\\n\"\n",
    "            \"Carefully read the user's question and the context provided.\\n\\n\"\n",
    "            \"Use context if available. Only refer to it when relevant, and never make assumptions about the user's problem.\\n\\n\"\n",
    "            \"You will guide the user through the troubleshooting process in a helpful, friendly manner.\\n\\n\"\n",
    "            \"🔵 Special Instructions:\\n\\n\"\n",
    "            \"1. If the user provides vague input, ask guiding questions (max 5 questions).\\n\"\n",
    "            \"2. If clarification is still needed, follow up with 1 small question.\\n\"\n",
    "            \"3. After 2 failed clarification attempts, suggest general causes based on past insights.\\n\\n\"\n",
    "            \"🔵 When responding:\\n\"\n",
    "            \"    - If the user remains unclear after clarification, suggest **General Causes**.\\n\"\n",
    "            \"    - If the user provides partial information, ask **1 short follow-up question**.\\n\"\n",
    "            \"    - Otherwise, keep the response brief and professional.\\n\\n\"\n",
    "            \"🔵 Additional Notes:\\n\"\n",
    "            \"- Maintain a friendly, professional, and clear tone.\\n\"\n",
    "            \"- Only ask 'Would you like me to continue?' if the response is longer than 300 words.\\n\"\n",
    "            \"- Ensure all responses are helpful and directed towards resolving the issue without making unnecessary assumptions.\\n\\n\"\n",
    "            \"🔵 Be respectful and polite, even when suggesting possible general causes or follow-ups.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cpu')\n",
    "\n",
    "# Step 1: Helper functions\n",
    "def build_context(top_matches):\n",
    "    return \"\\n\\n---\\n\\n\".join([\n",
    "        f\"{match['answer_body']}\" for match in top_matches\n",
    "    ])\n",
    "\n",
    "def summarize_old_messages(messages, preserve_n=20):\n",
    "    if len(messages) <= preserve_n:\n",
    "        return messages\n",
    "    \n",
    "    old_conversation = []\n",
    "    for msg in messages[1:-preserve_n]:\n",
    "        if msg['role'] in ['user', 'assistant']:\n",
    "            old_conversation.append(f\"{msg['role'].capitalize()}: {msg['content']}\")\n",
    "    \n",
    "    summary = \"🔵 Summary of earlier conversation:\\n\" + \"\\n\".join(\n",
    "        f\"- {line}\" for line in old_conversation\n",
    "    )\n",
    "    \n",
    "    summarized_messages = [\n",
    "        messages[0],\n",
    "        {\"role\": \"system\", \"content\": summary}\n",
    "    ] + messages[-preserve_n:]\n",
    "    \n",
    "    return summarized_messages\n",
    "\n",
    "def collapse_conversation_memory(messages):\n",
    "    conversation_summary = []\n",
    "    for msg in messages[1:]:\n",
    "        if msg['role'] in ['user', 'assistant']:\n",
    "            conversation_summary.append(f\"{msg['role'].capitalize()}: {msg['content']}\")\n",
    "    \n",
    "    final_summary = \"🔵 Resolved Issue Summary:\\n\" + \"\\n\".join(\n",
    "        f\"- {line}\" for line in conversation_summary\n",
    "    )\n",
    "    \n",
    "    collapsed_messages = [\n",
    "        messages[0],\n",
    "        {\"role\": \"system\", \"content\": final_summary}\n",
    "    ]\n",
    "    \n",
    "    return collapsed_messages\n",
    "\n",
    "def get_embedding(text):\n",
    "    return model.encode(text)\n",
    "\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "    embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "    return cosine_similarity(embedding1, embedding2)[0][0]\n",
    "\n",
    "# Step 2: State Variables\n",
    "initial_topic_embedding = None\n",
    "context_injected = False\n",
    "guiding_questions_done = False\n",
    "clarification_rounds = 0\n",
    "\n",
    "# Step 3: Core function\n",
    "def send_message(user_query):\n",
    "    global messages\n",
    "    global initial_topic_embedding\n",
    "    global context_injected\n",
    "    global guiding_questions_done\n",
    "    global clarification_rounds\n",
    "\n",
    "    user_embedding = get_embedding(user_query)\n",
    "\n",
    "    # Detect topic change only if input is substantial\n",
    "    if initial_topic_embedding is None:\n",
    "        initial_topic_embedding = user_embedding\n",
    "    elif len(user_query.split()) > 5:  # Ignore very short replies like \"it\", \"1.8.19\"\n",
    "        similarity = calculate_similarity(initial_topic_embedding, user_embedding)\n",
    "        if similarity < 0.5:\n",
    "            print(f\"🔄 Major topic change detected (similarity {similarity:.2f}). Retriggering RAG...\")\n",
    "            context_injected = False\n",
    "            initial_topic_embedding = user_embedding\n",
    "            guiding_questions_done = False\n",
    "            clarification_rounds = 0\n",
    "\n",
    "    if not context_injected:\n",
    "        top_matches = search_embeddings(user_query)\n",
    "        min_score_threshold = 0.4\n",
    "\n",
    "        if not top_matches or top_matches[0]['score'] < min_score_threshold:\n",
    "            context_message = {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"🔵 Context Update:\\n\\n\"\n",
    "                    \"No strong matching past issues found.\\n\\n\"\n",
    "                    \"(Answer politely based on general knowledge.)\"\n",
    "                )\n",
    "            }\n",
    "        else:\n",
    "            context = build_context(top_matches)\n",
    "            quoted_context = context.replace('\\n', '\\n> ')\n",
    "            context_message = {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"🔵 Context Update:\\n\\n\"\n",
    "                    \"Summaries of past similar issues (use carefully):\\n\\n\"\n",
    "                    f\"> {quoted_context}\\n\\n\"\n",
    "                    \"(Only use if truly matching.)\"\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        messages.append(context_message)\n",
    "        context_injected = True\n",
    "\n",
    "    # Step 2: Behavior Control\n",
    "    user_text_lower = user_query.lower()\n",
    "    response_behavior = \"normal\"\n",
    "\n",
    "    vague_keywords = [\"problem\", \"idk\", \"not sure\", \"nothing working\"]\n",
    "\n",
    "    if any(vague in user_text_lower for vague in vague_keywords):\n",
    "        if not guiding_questions_done:\n",
    "            response_behavior = \"guiding_questions\"\n",
    "            guiding_questions_done = True\n",
    "        else:\n",
    "            clarification_rounds += 1\n",
    "            if clarification_rounds >= 2:\n",
    "                response_behavior = \"general_causes\"\n",
    "            else:\n",
    "                response_behavior = \"follow_up_question\"\n",
    "\n",
    "    # Step 3: Insert dynamic system message to guide LLM\n",
    "    if response_behavior == \"guiding_questions\":\n",
    "        behavior_instruction = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"🔵 Special Behavior Instruction:\\n\"\n",
    "                \"The user seems unsure. Kindly ask around 5 short guiding questions to clarify their issue.\"\n",
    "            )\n",
    "        }\n",
    "    elif response_behavior == \"follow_up_question\":\n",
    "        behavior_instruction = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"🔵 Special Behavior Instruction:\\n\"\n",
    "                \"The user provided partial information. Ask 1 short follow-up question to get more detail.\"\n",
    "            )\n",
    "        }\n",
    "    elif response_behavior == \"general_causes\":\n",
    "        behavior_instruction = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"🔵 Special Behavior Instruction:\\n\"\n",
    "                \"The user remains unclear after multiple tries. Politely suggest some general possible causes based on common troubleshooting experience.\"\n",
    "            )\n",
    "        }\n",
    "    else:\n",
    "        behavior_instruction = None\n",
    "\n",
    "    if behavior_instruction:\n",
    "        messages.append(behavior_instruction)\n",
    "\n",
    "    # Step 4: Add user query\n",
    "    messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "    # Step 5: Build payload\n",
    "    payload = {\n",
    "        \"model\": \"model\",  # <-- Keep \"model\" exactly as you requested\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"presence_penalty\": 0.2,\n",
    "        \"frequency_penalty\": 0.2,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(infer_url, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        output_body = response.json()\n",
    "        generated_response = output_body['choices'][0]['message']['content']\n",
    "        \n",
    "        tokens_generated = len(generated_response.split())\n",
    "        if tokens_generated > 300:\n",
    "            generated_response += \"\\n\\nWould you like me to continue?\"\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": generated_response.strip()})\n",
    "\n",
    "        if len(messages) > 30:\n",
    "            messages = summarize_old_messages(messages, preserve_n=15)\n",
    "        \n",
    "        return generated_response.strip()\n",
    "    else:\n",
    "        return f\"⚠️ Error {response.status_code}: {response.text}\"\n",
    "\n",
    "# Step 4: User interaction loop\n",
    "print(\"🗨️ Welcome to GitHub Helpdesk Chatbot!\")\n",
    "print(\"Type 'exit' to end the conversation.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"👤 You: \")\n",
    "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"👋 Ending the chat. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    if any(keyword in user_query.lower() for keyword in [\"solved\", \"fixed\", \"thank you\", \"thanks\", \"resolved\", \"issue resolved\"]):\n",
    "        print(\"✅ Issue marked as solved! Collapsing conversation memory...\")\n",
    "        messages = collapse_conversation_memory(messages)\n",
    "        continue\n",
    "    \n",
    "    bot_reply = send_message(user_query)\n",
    "    print(f\"🤖 Assistant: {bot_reply}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f9ece-e9cf-44e2-a8a2-73160186aee8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Request Function</h1>\n",
    "\n",
    "Build and submit the REST request. \n",
    "\n",
    "Note: You submit the data in the same format that you used for an ONNX inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b9386f-683a-4880-b780-c40bec3ab9f8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def rest_request(prompt):\n",
    "    json_data = {\n",
    "        \"model\": \"llm\",\n",
    "        \"prompt\": [\n",
    "            prompt\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"logprobs\": 0,\n",
    "        \"echo\": False,\n",
    "        \"stop\": [\n",
    "            \"string\"\n",
    "        ],\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"best_of\": 1,\n",
    "        \"user\": \"string\",\n",
    "        \"top_k\": -1,\n",
    "        \"ignore_eos\": False,\n",
    "        \"use_beam_search\": False,\n",
    "        \"stop_token_ids\": [\n",
    "            0\n",
    "        ],\n",
    "        \"skip_special_tokens\": True,\n",
    "        \"spaces_between_special_tokens\": True,\n",
    "        \"repetition_penalty\": 1,\n",
    "        \"min_p\": 0,\n",
    "        \"include_stop_str_in_output\": False,\n",
    "        \"length_penalty\": 1\n",
    "    }\n",
    "\n",
    "    response = requests.post(infer_url, json=json_data, verify=False)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ad16ac-23da-48bd-9796-f8e4cacae981",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='model-predictor.minio.svc.cluster.local', port=80): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fadbd8a11d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.11/http/client.py:1298\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.11/http/client.py:1058\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.11/http/client.py:996\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 996\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fadbd8a11d0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='model-predictor.minio.svc.cluster.local', port=80): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fadbd8a11d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mrest_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat accelerators are supported in openshift AI?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m prediction\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mrest_request\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrest_request\u001b[39m(prompt):\n\u001b[1;32m      5\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m     }\n\u001b[0;32m---> 38\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfer_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='model-predictor.minio.svc.cluster.local', port=80): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fadbd8a11d0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "prediction = rest_request(\"What accelerators are supported in openshift AI?\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c336d-44a7-47cc-81ea-ab14b980fc27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
