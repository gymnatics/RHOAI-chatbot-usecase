{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24763475a3d8998c",
   "metadata": {},
   "source": [
    "<h1>Model Inference with RAG</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c004acc-13cd-4917-8480-592c7c2d623b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Change that following variable settings match your deployed model's *Inference endpoint*. for example: \n",
    "\n",
    "```\n",
    "infer_endpoint = \"https://model-vllm.apps.clusterx.sandboxx.opentlc.com\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e51a248-1ca8-496a-9fe3-9306b1974b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from file 3_RAG_with_Elastic\n",
    "# client = Elasticsearch(['https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'], basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), verify_certs=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eadce4d-0b69-4f0a-aff7-5f888e85e4f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Model Inference</h1>\n",
    "\n",
    "<h3>using: /v1/completions endpoint</h3>\n",
    "<h4>*More for Single-shot prompts, single-turn Q&A or document completion</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0de65d02-84a6-4cff-882e-551cdd42b486",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_endpoint = \"http://model-predictor.minio.svc.cluster.local:8080\" #Change infer endpoint here\n",
    "infer_url = f\"{infer_endpoint}/v1/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fc76424-b6cb-4ac9-a4d5-54a3da27d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"model\",\"object\":\"model\",\"created\":1744796825,\"owned_by\":\"vllm\",\"root\":\"/mnt/models\",\"parent\":null,\"max_model_len\":4096,\"permission\":[{\"id\":\"modelperm-d13b38a9a04c4f88b2feb40fb62cd77a\",\"object\":\"model_permission\",\"created\":1744796825,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "# get the model name \n",
    "# for testing only\n",
    "!curl http://model-predictor.minio.svc.cluster.local:8080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bdff6a7-03c9-418d-ae55-22efbbc32cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenShift is a container application platform built around a core of application containers powered by Docker, with an operating system layer provided by Project Atomic (RHEL Atomic or CentOS Atomic), and orchestration and management provided by Kubernetes. It adds features that make it easier to work with, including a web console, developer CLI tools, integrated source-to-image (S2I) build support, pod templates for common use cases, and more.\n",
      "Kubernetes is an open source project for automating deployment, scaling, and operations of application containers across clusters of hosts. It groups containers that make up an application into logical units for easy management and discovery.\n"
     ]
    }
   ],
   "source": [
    "# Test for hyperparameter tuning \n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"prompt\": \"What's the difference between OpenShift and Kubernetes?\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['text']\n",
    "print(generated_response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fa17d77-2268-4551-bcdb-355608dd0fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenShift is a container application platform built around a core of application containers orchestrated and managed by Kubernetes. It adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, deployment, and scaling. In essence, OpenShift is a distribution of Kubernetes with additional features and tools for developers and operations teams.\n",
      "\n",
      "Kubernetes, on the other hand, is an open-source container orchestration system for automating the deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery.\n",
      "\n",
      "So, while Kubernetes is the underlying technology for managing containers, OpenShift is a complete platform that builds on Kubernetes to provide a more comprehensive solution for developing, deploying, and managing containerized applications.\n"
     ]
    }
   ],
   "source": [
    "# Test for hyperparameter tuning \n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"prompt\": \"What's the difference between OpenShift and Kubernetes?\",\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['text']\n",
    "print(generated_response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61cf4b-12c7-45ab-bb8b-180bac0cdd4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Model Inference</h1>\n",
    "\n",
    "<h3>using: /v1/chat/completions endpoint</h3>\n",
    "<h3>*More for chat-tuned, multi-turn conversations, model behaving like assistant</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea5b6587-e26d-4885-91a5-e774e479c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO DO into the Prompt Template\n",
    "# preserve messages history \n",
    "# keep older messages if they are essential for content \n",
    "# truncate long histories by summarizing/dropping older exchanges \n",
    "\n",
    "# Prompt token - is there a max? \n",
    "\n",
    "# create a Python class to connect to FE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1adf3582-4d44-4fd2-80f9-0fb79fca5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_endpoint = \"http://model-predictor.minio.svc.cluster.local:8080\" #Change infer endpoint here\n",
    "infer_url = f\"{infer_endpoint}/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4360dbaf-baa5-41b5-9e68-078f3b9c0dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"model\",\"object\":\"model\",\"created\":1745201776,\"owned_by\":\"vllm\",\"root\":\"/mnt/models\",\"parent\":null,\"max_model_len\":4096,\"permission\":[{\"id\":\"modelperm-0b65155b882b4135a6fb53b11cf4617d\",\"object\":\"model_permission\",\"created\":1745201776,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "# get the model name \n",
    "# for testing only\n",
    "!curl http://model-predictor.minio.svc.cluster.local:8080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a68837a-cf3d-464b-9a9a-d816c55daca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenShift and Kubernetes are both popular containerization platforms, but they have some key differences:\n",
      "\n",
      "1. **Origin and Ownership**:\n",
      "   - Kubernetes (K8s) is an open-source platform developed by Google based on their internal system Borg. It was donated to the Cloud Native Computing Foundation (CNCF).\n",
      "   - OpenShift is a commercial product from Red Hat (now part of IBM), built on top of Kubernetes. It's also available as an open-source project called OKD (OpenShift Origin).\n",
      "\n",
      "2. **Ease of Use**:\n",
      "   - Kubernetes requires more manual configuration and management, making it more complex for beginners or smaller teams.\n",
      "   - OpenShift provides additional features that simplify deployment, scaling, and management of applications, thus making it easier to use, especially for those new to container orchestration.\n",
      "\n",
      "3. **Built-in Features**:\n",
      "   - OpenShift includes several tools out-of-the-box such as a web console, integrated CI/CD pipelines with Jenkins, built-in source-to-image builds, and an integrated registry. These require additional setup in a vanilla Kubernetes environment.\n",
      "   - Kubernetes itself is more barebones and modular, allowing you to add components like Helm for package management and Jenkins for CI/CD pipelines.\n",
      "\n",
      "4. **Security**:\n",
      "   - OpenShift has stronger security features due to its enterprise focus, including network policies, secret management, and role-based access control (RBAC).\n",
      "   - While Kubernetes also offers robust security features, setting them up might need more expertise compared to OpenShift.\n",
      "\n",
      "5. **Support and Community**:\n",
      "   - As a commercial product backed by Red Hat/IBM, OpenShift comes with professional support options. Its community is also quite active and focused on enterprise usage.\n",
      "   - Kubernetes has a vast community due to its open-source nature and widespread adoption across industries. This can be beneficial for finding solutions online but might lack the direct support offered by OpenShift.\n",
      "\n",
      "In summary, while Kubernetes is highly flexible and customizable, OpenShift provides a more user-friendly experience with added features out of the box that cater particularly well to enterprise needs. The choice between the two often depends on your specific requirements, technical expertise, and whether you prefer a more hands-on approach or a more managed solution.\n"
     ]
    }
   ],
   "source": [
    "user_question_one = \"What's the difference between OpenShift & Kubernetes?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chat bot assistant for helping people with their queries.\"},\n",
    "    {\"role\": \"user\", \"content\": user_question_one}\n",
    "]\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['message']['content']\n",
    "print(generated_response.strip())\n",
    "\n",
    "# add to the messages list \n",
    "messages.append({\"role\": \"system\", \"content\": generated_response.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8d0596c-62e3-4b19-b54f-a85203d985b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing AI with Retrieval-Augmented Generation (RAG) using Elasticsearch as a vector database involves several steps. Here's a simplified guide:\n",
      "\n",
      "1. **Data Preparation**: \n",
      "   First, you'll need a dataset relevant to your task. This could be text documents, articles, or any other form of data you want your model to generate responses from. \n",
      "\n",
      "2. **Vectorization**:\n",
      "   Before you can store these texts in Elasticsearch, they need to be converted into numerical vectors. You can use pre-trained language models like Sentence Transformers or Universal Sentence Encoder to convert text into high dimensional vectors (embeddings). These embeddings capture semantic meaning, so similar texts will have similar vectors.\n",
      "\n",
      "   Here's a simple example using Hugging Face's Sentence Transformer library in Python:\n",
      "\n",
      "   ```python\n",
      "   from sentence_transformers import SentenceTransformer\n",
      "\n",
      "   model = SentenceTransformer('all-MiniLM-L6-v2')  # Load pre-trained model\n",
      "   sentences = [\"This is the first sentence\", \"This is the second sentence\"]  # Your data\n",
      "   embeddings = model.encode(sentences)  # Get sentence embeddings\n",
      "   ```\n",
      "\n",
      "3. **Setting Up Elasticsearch**:\n",
      "   Install and set up Elasticsearch on your machine or cluster. Make sure it's running before proceeding. You'll also need the official Elasticsearch xPack and its Machine Learning features enabled.\n",
      "\n",
      "4. **Indexing Vectors in Elasticsearch**:\n",
      "   To index your embeddings in Elasticsearch, you can use the `elastic-rag` library which provides utilities for this purpose. Here's a basic example:\n",
      "\n",
      "   ```python\n",
      "   from elastic_rag import ElasticRagClient\n",
      "\n",
      "   client = ElasticRagClient(hosts=\"localhost:9200\")  # Replace with your ES URL\n",
      "   client.index_vectors(\"my_dataset\", \"my_collection\", embeddings, metadata=sentences)  # Index embeddings and associated metadata\n",
      "   ```\n",
      "\n",
      "5. **Querying with RAG**:\n",
      "   Now you can query your vector database using RAG approach. Given an input prompt, you first encode it into a vector, then find the most similar vectors in your database (retrieval), and finally use these retrieved documents to augment the generation of a response (generation).\n",
      "\n",
      "   Here's a simplistic example using Gensim's `Doc2Vec` for generating responses:\n",
      "\n",
      "   ```python\n",
      "   from gensim.models import Doc2Vec\n",
      "   \n",
      "   # Assume 'model' is trained on your corpus and 'vectorizer' converts text to vectors\n",
      "   input_prompt = \"Your input prompt here\"\n",
      "   input_vector = vectorizer[input_prompt]\n",
      "   \n",
      "   # Retrieve nearest neighbors from Elasticsearch\n",
      "   nearest_neighbors = client.query_vectors(\"my_dataset\", \"my_collection\", input_vector, n=5)  # Adjust n as needed\n",
      "   \n",
      "   # Augment response generation using retrieved docs\n",
      "   augmented_response = generate_response(model, nearest_neighbors)  # Implement this function according to your needs\n",
      "   \n",
      "   print(augmented_response)\n",
      "   ```\n",
      "   \n",
      "6. **Generate Response**: \n",
      "   In this step, you'd typically fine-tune a sequence-to-sequence model (like T5 or BART) on your dataset to generate human-like text given an input prompt and context (in this case, the retrieved documents). The exact implementation would depend on the specific model and framework you're using (e.g., PyTorch, TensorFlow).\n",
      "\n",
      "Remember, this is a high-level overview and each step involves many details that you'd need to work out depending on your specific use case and environment. Also, ensure that you handle exceptions and edge cases appropriately in production code. \n",
      "\n",
      "Lastly, keep in mind that working with large datasets might require significant computational resources and careful tuning of parameters for optimal performance.\n"
     ]
    }
   ],
   "source": [
    "# ongoing conversations \n",
    "\n",
    "user_question_two = \"I'm interested in implementing AI with RAG approach, tell me more about how i can do it with elastic as the vector database\"\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_question_two})\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"model\",\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3, # Controls randomness, lower temp for factuality \n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1, # Number of completions to generate\n",
    "    \"repetition_penalty\": 1.1, # Penalize repeated tokens (1 = no penalty)\n",
    "    \"presence_penalty\": 0.2, # Discourage mentioning same concepts again\n",
    "    \"frequency_penalty\": 0.2, # Discourage repeating the *same words* too frequently\n",
    "    \"stream\": False # If True, stream tokens back (like a live typewriter)\n",
    "}\n",
    "\n",
    "response = requests.post(infer_url, json=payload)\n",
    "\n",
    "# prints the whole response json\n",
    "# print(response.json())\n",
    "\n",
    "output_body = response.json()\n",
    "generated_response = output_body['choices'][0]['message']['content']\n",
    "print(generated_response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2a831-9d78-4159-b393-58865906f2a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Create Embeddings in Elastic Vector Database</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "577e04b8-11be-4c2a-a97e-b45e018127ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/app-root/lib64/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: elasticsearch in /opt/app-root/lib64/python3.11/site-packages (8.18.0)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /opt/app-root/lib64/python3.11/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in /opt/app-root/lib64/python3.11/site-packages (from elasticsearch) (8.17.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/app-root/lib64/python3.11/site-packages (from elasticsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib64/python3.11/site-packages (from elasticsearch) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /opt/app-root/lib64/python3.11/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.3.0)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil->elasticsearch) (1.17.0)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/app-root/lib64/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/app-root/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.10)\n"
     ]
    }
   ],
   "source": [
    "# install dependencies & imports\n",
    "!pip install sentence-transformers elasticsearch pandas tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from tqdm import tqdm\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d482934a-6c89-4fc9-b0f7-c448676dd479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   issue_id  answer_id author              creation_time  \\\n",
      "0    456983          0  almet  2010-12-05T17:31:55+00:00   \n",
      "1    456983          1  Gui13  2010-12-05T17:43:43+00:00   \n",
      "2    456983          2  almet  2010-12-05T17:45:28+00:00   \n",
      "3    456983          3  almet  2010-12-14T15:57:04+00:00   \n",
      "4    456983          4  Gui13  2010-12-21T21:02:19+00:00   \n",
      "\n",
      "                                          issue_body  \\\n",
      "0  Hey, Say I add an image in my article, giving ...   \n",
      "1  Hey, Say I add an image in my article, giving ...   \n",
      "2  Hey, Say I add an image in my article, giving ...   \n",
      "3  Hey, Say I add an image in my article, giving ...   \n",
      "4  Hey, Say I add an image in my article, giving ...   \n",
      "\n",
      "                                         answer_body  \n",
      "0  If we do add the SITEURL value, it can break b...  \n",
      "1  A bit hackish for sure, but I'm not yet good e...  \n",
      "2  okay, let's go for that so. Will do that when ...  \n",
      "3  arnaud is currently working on that, see his f...  \n",
      "4                     Should this bug be closed now?  \n",
      "\n",
      "\n",
      "['issue_id', 'answer_id', 'author', 'creation_time', 'issue_body', 'answer_body']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_helpdesk_data.csv\", low_memory=False)\n",
    "print(df.head())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "321accf0-c0b0-4a7e-9c33-63f1494258f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing - create a smaller csv\n",
    "df = pd.read_csv(\"cleaned_helpdesk_data.csv\")\n",
    "df_sample = df.head(20)\n",
    "df_sample.to_csv(\"helpdesk_small_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "70712625-c421-4644-87f4-a407bc3d0eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'elasticsearch-sample-es-default-1', 'cluster_name': 'elasticsearch-sample', 'cluster_uuid': 'nr3Sh4ArQcqFm90rHd5Smw', 'version': {'number': '8.17.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2b6a7fed44faa321997703718f07ee0420804b41', 'build_date': '2024-12-11T12:08:05.663969764Z', 'build_snapshot': False, 'lucene_version': '9.12.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n",
      "✅ Embeddings stored successfully in Elasticsearch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:403: SecurityWarning: Connecting to 'https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LOGIC\n",
    "# ideally, all the contents of 'cleaned_body' field from the continous answer_ids grouped under the same issue_id should be in a single vector embedding \n",
    "\n",
    "# Load your job dataset\n",
    "df = pd.read_csv(\"helpdesk_small_sample.csv\")\n",
    "\n",
    "# not all values in clean_body are strings, thus throwing TypeError if this line is not runned \n",
    "df[\"answer_body\"] = df[\"answer_body\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Group answers by issue_id and concatenate them in order\n",
    "grouped = (\n",
    "    df.sort_values(by=[\"issue_id\", \"answer_id\"])\n",
    "      .groupby(\"issue_id\")\n",
    "      .agg({\n",
    "          \"issue_body\": \"first\",\n",
    "          \"answer_body\": lambda x: \" \".join(x)\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Combine issue_body and answer_body into a single string for embedding\n",
    "grouped[\"full_text\"] = grouped[\"issue_body\"] + \" \" + grouped[\"answer_body\"]\n",
    "\n",
    "# Generate embeddings\n",
    "grouped[\"embedding\"] = model.encode(grouped[\"full_text\"]).tolist()\n",
    "\n",
    "# grouped = df.sort_values(by=[\"issue_id\", \"answer_id\"]).groupby(\"issue_id\")[\"clean_body\"].apply(lambda texts: \" \".join(texts)).reset_index()\n",
    "\n",
    "\n",
    "# Embedding Model \n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # faced out of memory error \n",
    "# model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\", device=\"cuda\")  # lighter version\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cpu')\n",
    "\n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "print(es.info())\n",
    "\n",
    "index_name = \"helpdesk-embeddings\"\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"issue_id\": {\"type\": \"keyword\"},\n",
    "                    \"issue_body\": {\"type\": \"text\"},\n",
    "                    \"answer_body\": {\"type\": \"text\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": len(grouped[\"embedding\"].iloc[0]),\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Prepare docs for bulk indexing\n",
    "def create_docs(data):\n",
    "    for _, row in data.iterrows():\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": row[\"issue_id\"],\n",
    "            \"_source\": {\n",
    "                \"issue_id\": row[\"issue_id\"],\n",
    "                \"issue_body\": row[\"issue_body\"],\n",
    "                \"answer_body\": row[\"answer_body\"],\n",
    "                \"embedding\": row[\"embedding\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Bulk upload\n",
    "bulk(es, create_docs(grouped))\n",
    "print(\"✅ Embeddings stored successfully in Elasticsearch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "21eb03fd-5651-4ac7-b465-f9ace6dc7210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available indices: ['rhoai_index', 'helpdesk-embeddings']\n",
      "Num of embeddings in index: 5\n",
      "ID: 456983\n",
      "ID: 648603\n",
      "ID: 798136\n",
      "ID: 814067\n",
      "ID: 984126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## for debugging/testing purposes only \n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# view available embedding indices \n",
    "indices = es.indices.get_alias().keys()\n",
    "print(\"Available indices:\", list(indices))\n",
    "\n",
    "# view all stored embeddings \n",
    "results = es.search(\n",
    "    index=\"helpdesk-embeddings\",\n",
    "    size=5,  # adjust as needed\n",
    "    _source=[\"embedding\"]  # only return embedding field\n",
    ")\n",
    "\n",
    "# view the number of documents in the stored index \n",
    "count = es.count(index=\"helpdesk-embeddings\")[\"count\"]\n",
    "print(f\"Num of embeddings in index: {count}\")\n",
    "\n",
    "\n",
    "for doc in results[\"hits\"][\"hits\"]:\n",
    "    print(f\"ID: {doc['_id']}\")\n",
    "    # print(\"Embedding:\", doc[\"_source\"][\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4668850a-653f-4f22-9a9a-f0ce82a366dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index deleted.\n",
      "\n",
      "Available indices: ['rhoai_index']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:403: SecurityWarning: Connecting to 'https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# delete the entire index after checking \n",
    "# ONLY RUN TO DELETE THE ENTIRE INDEX CREATED - SO THAT IT IS A CLEAN STATE IN VECTOR DB\n",
    "\n",
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "es.indices.delete(index=\"helpdesk-embeddings\", ignore_unavailable=True)\n",
    "print(\"Index deleted.\\n\")\n",
    "\n",
    "# view available embedding indices \n",
    "indices = es.indices.get_alias().keys()\n",
    "print(\"Available indices:\", list(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a5b52-dad1-4f56-ba1a-8f1e79107c86",
   "metadata": {},
   "source": [
    "<h1>Model Reference with RAG</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4b153-1630-46d1-97ef-f28ca7b92e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elasticsearch client \n",
    "es = Elasticsearch(\n",
    "    hosts=[\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\"], \n",
    "    basic_auth=('elastic', '1rdzVXU8i8F9T8hs1p6c78d6'), \n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device='cpu')\n",
    "\n",
    "\n",
    "# get top 3 most relevant embeddings \n",
    "def search_embeddings(query, top_n=3):\n",
    "    # Step 1: Embed the query using the same model\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "\n",
    "    # Step 2: Search the Elasticsearch index for the closest vector matches\n",
    "    response = es.search(\n",
    "        index=\"helpdesk-embeddings\",  \n",
    "        body={\n",
    "            \"size\": top_n,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"embedding\": {\n",
    "                        \"vector\": query_embedding,\n",
    "                        \"k\": top_n\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    relevant_answers = []\n",
    "\n",
    "    for hit in hits:\n",
    "        issue_id = hit[\"_source\"][\"issue_id\"]\n",
    "        answer_body = hit[\"_source\"][\"answer_body\"]\n",
    "        score = hit[\"_score\"]\n",
    "        relevant_answers.append({\n",
    "            \"issue_id\": issue_id,\n",
    "            \"answer_body\": answer_body,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    return relevant_answers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1b0b2-e357-4cd9-81b7-1dd45845ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from template (ignore first)\n",
    "\n",
    "\n",
    "def retrieve_relevant_context(question, es_url=\"https://elasticsearch-sample-elasticsearch.apps.rosa-t59w8.oufo.p1.openshiftapps.com\", index_name=\"helpdesk-embeddings\"):\n",
    "    query = {\n",
    "        \"size\": 3,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": question\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.get(f\"{es_url}/{index_name}/_search\", json=query)\n",
    "    hits = response.json()[\"hits\"][\"hits\"]\n",
    "    context = \"\\n\\n\".join([hit[\"_source\"][\"content\"] for hit in hits])\n",
    "    return context\n",
    "\n",
    "def rag_chat(question, infer_url, system_prompt=\"You are a helpful assistant.\"):\n",
    "    # Step 1: Retrieve relevant knowledge\n",
    "    context = retrieve_relevant_context(question)\n",
    "\n",
    "    # Step 2: Build the chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"model\",  # replace with actual model name or ID\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 512\n",
    "    }\n",
    "\n",
    "    # Step 3: Get response from model\n",
    "    response = requests.post(f\"{infer_url}/v1/chat/completions\", json=payload)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Example usage\n",
    "infer_endpoint = \"http://model-endpoint\"  # change to your endpoint\n",
    "user_question = \"What qualifications are required for a software engineer?\"\n",
    "response = rag_chat(user_question, infer_endpoint)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f9ece-e9cf-44e2-a8a2-73160186aee8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Request Function</h1>\n",
    "\n",
    "Build and submit the REST request. \n",
    "\n",
    "Note: You submit the data in the same format that you used for an ONNX inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b9386f-683a-4880-b780-c40bec3ab9f8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def rest_request(prompt):\n",
    "    json_data = {\n",
    "        \"model\": \"llm\",\n",
    "        \"prompt\": [\n",
    "            prompt\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"logprobs\": 0,\n",
    "        \"echo\": False,\n",
    "        \"stop\": [\n",
    "            \"string\"\n",
    "        ],\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"best_of\": 1,\n",
    "        \"user\": \"string\",\n",
    "        \"top_k\": -1,\n",
    "        \"ignore_eos\": False,\n",
    "        \"use_beam_search\": False,\n",
    "        \"stop_token_ids\": [\n",
    "            0\n",
    "        ],\n",
    "        \"skip_special_tokens\": True,\n",
    "        \"spaces_between_special_tokens\": True,\n",
    "        \"repetition_penalty\": 1,\n",
    "        \"min_p\": 0,\n",
    "        \"include_stop_str_in_output\": False,\n",
    "        \"length_penalty\": 1\n",
    "    }\n",
    "\n",
    "    response = requests.post(infer_url, json=json_data, verify=False)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ad16ac-23da-48bd-9796-f8e4cacae981",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='model-predictor.minio.svc.cluster.local', port=80): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fadbd8a11d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.11/http/client.py:1298\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.11/http/client.py:1058\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.11/http/client.py:996\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 996\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fadbd8a11d0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='model-predictor.minio.svc.cluster.local', port=80): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fadbd8a11d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mrest_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat accelerators are supported in openshift AI?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m prediction\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mrest_request\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrest_request\u001b[39m(prompt):\n\u001b[1;32m      5\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m     }\n\u001b[0;32m---> 38\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfer_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='model-predictor.minio.svc.cluster.local', port=80): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fadbd8a11d0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "prediction = rest_request(\"What accelerators are supported in openshift AI?\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c336d-44a7-47cc-81ea-ab14b980fc27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
